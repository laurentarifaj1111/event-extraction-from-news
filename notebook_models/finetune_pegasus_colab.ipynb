{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegasus Fine-tuning and Inference in Google Colab\n",
    "\n",
    "This notebook fine-tunes Pegasus model on your dataset and runs inference pipeline.\n",
    "\n",
    "## Setup\n",
    "1. Upload your data to Google Drive\n",
    "2. Update the `DRIVE_DATA_PATH` variable below\n",
    "3. Run all cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'google/pegasus-large'\n",
    "# MODEL = 'google/pegasus-cnn_dailymail'\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results-pegasus/2k_samples'\n",
    "MAX_LENGTH = 1024 # Maximum context length to consider while preparing dataset.\n",
    "epoch_metrics = []\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/10k_samples\"   # UPDATE PATH\n",
    "CLEAN_TEXT_COLUMN='article'\n",
    "SUMMARY_COLUMN='highlights'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration,  Trainer, TrainingArguments,\n",
    "    PegasusTokenizer,EarlyStoppingCallback,T5ForConditionalGeneration, T5Tokenizer,\n",
    "    PegasusXForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "  \"\"\"\n",
    "  Prepare configurations and base model for fine-tuning\n",
    "  \"\"\"\n",
    "  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "  if freeze_encoder:\n",
    "    for param in model.model.encoder.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  if val_dataset is not None:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=OUT_DIR,           # output directory\n",
    "      num_train_epochs=10,           # total number of training epochs\n",
    "      per_device_train_batch_size=18,   # batch size per device during training, can increase if memory allows\n",
    "      per_device_eval_batch_size=16,    # batch size for evaluation, can increase if memory allows\n",
    "      save_steps=100,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=10,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      eval_strategy='steps',     # evaluation strategy to adopt during training\n",
    "      eval_steps=100,                  # number of update steps before evaluation\n",
    "      warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir='./logs',            # directory for storing logs\n",
    "      logging_steps=100,\n",
    "      learning_rate=0.00003,\n",
    "      predict_with_generate=True,\n",
    "          bf16=True,\n",
    "    fp16=False,\n",
    "    tf32=True,\n",
    "\n",
    "    torch_compile=True,\n",
    "      report_to='tensorboard',\n",
    "          metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      eval_dataset=val_dataset,            # evaluation dataset\n",
    "      tokenizer=tokenizer,\n",
    "      compute_metrics=compute_metrics,\n",
    "      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=OUT_DIR,           # output directory\n",
    "      num_train_epochs=10,           # total number of training epochs\n",
    "      per_device_train_batch_size=8,   # batch size per device during training, can increase if memory allows\n",
    "      save_steps=100,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir='./logs',            # directory for storing logs\n",
    "      logging_steps=100,\n",
    "      learning_rate=0.00003,\n",
    "      predict_with_generate=True,\n",
    "      report_to='tensorboard',\n",
    "      eval_strategy='steps',     # evaluation strategy to adopt during training\n",
    "      eval_steps=100,                  # number of update steps before evaluation\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    tf32=True,\n",
    "\n",
    "    torch_compile=True,\n",
    "          metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      tokenizer=tokenizer,\n",
    "      compute_metrics=compute_metrics,\n",
    "      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainerCallback\n",
    "import time\n",
    "writer = SummaryWriter(log_dir=OUT_DIR)\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])  # len(self.labels)\n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        return control\n",
    "\n",
    "class GpuLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            self.writer.add_scalar(\"gpu_memory_gb\", gpu_mem, state.global_step)\n",
    "        return control\n",
    "\n",
    "\n",
    "def prepare_data(model_name,\n",
    "                 train_texts, train_labels,\n",
    "                 val_texts=None, val_labels=None,\n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    # predictions, labels = eval_pred.predictions[0], eval_pred.label_ids[0]\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "\n",
    "    # Decode the predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels before decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # NEW: Log to TensorBoard\n",
    "    for k, v in rouge_result.items():\n",
    "        writer.add_scalar(f\"eval/{k}\", v, trainer.state.global_step)\n",
    "    # Print rounded values\n",
    "    # pprint.print({k: round(v, 4) for k, v in rouge_result.items()})\n",
    "\n",
    "    # Must return a *dict*, not a set\n",
    "    return {k: v for k, v in rouge_result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Pegasus Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "  # train_texts, train_labels = dataset['train']['document'][:1000], dataset['train']['summary'][:1000]\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/train.csv\").head(2000)\n",
    "val_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/val.csv\").head(400)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_valid = Dataset.from_pandas(val_df)\n",
    "train_texts, train_labels = dataset_train[CLEAN_TEXT_COLUMN][:2000], dataset_train[SUMMARY_COLUMN][:2000]\n",
    "valid_texts, valid_labels = dataset_valid[CLEAN_TEXT_COLUMN][:2000], dataset_valid[SUMMARY_COLUMN][:2000]\n",
    "\n",
    "\n",
    "\n",
    "# train_texts = train_df[CLEAN_TEXT_COLUMN]\n",
    "# train_labels = train_df[SUMMARY_COLUMN]\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Val:\", len(valid_texts))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, tokenizer = prepare_data(MODEL, train_texts, train_labels, valid_texts, valid_labels)\n",
    "trainer = prepare_fine_tuning(MODEL, tokenizer, train_dataset, val_dataset=val_dataset)\n",
    "trainer.train()\n",
    "trainer.add_callback(GpuLoggerCallback(writer))\n",
    "start = time.time()\n",
    "history = trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "\n",
    "# dataset_train = Dataset.from_pandas(train_df)\n",
    "# dataset_valid = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "path = f'results'\n",
    "last_ckpt = get_last_checkpoint(path)\n",
    "model_path = last_ckpt if last_ckpt else OUT_DIR\n",
    "\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = PegasusForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# model = T5ForConditionalGeneration.from_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # 1. Tokenize properly (returns attention mask too)\n",
    "    encoded = tokenizer(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # 2. Move everything to the same device as the model\n",
    "    device = model.device\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    # 3. Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        **encoded,\n",
    "        max_length=128,         # not 50 â†’ 50 is too short for news\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=1.1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # 4. Decode\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "nli_tok = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(device)\n",
    "\n",
    "def hallucination_rate(summary, source):\n",
    "    sentences = nltk.sent_tokenize(summary)\n",
    "    hallucinated = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = nli_tok.encode_plus(source, sent, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        logits = nli_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        contradiction = probs[0].item()\n",
    "        entailment = probs[2].item()\n",
    "\n",
    "        if contradiction > entailment:\n",
    "            hallucinated += 1\n",
    "\n",
    "    return hallucinated / len(sentences)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/test.csv\")\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!rm -rf '/content/results-pegasus/2k_samples/checkpoint-1100'\n",
    "# !rm -rf '/content/results_t5_large_regularized'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# Create a local directory for mounting\n",
    "# !mkdir results_t5base\n",
    "\n",
    "# Mount the GCS bucket\n",
    "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
    "# !gcsfuse --implicit-dirs models_checkpoint results_t5base\n",
    "\n",
    "!gsutil -m cp -r /content/results-pegasus/ gs://pegasus_2k_model/models/results_pegasus-large/results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
