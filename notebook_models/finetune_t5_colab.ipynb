{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL = 't5-large'\n",
    "BATCH_SIZE = 2\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results_t5_large/10k_samples'\n",
    "MAX_LENGTH = 1024 # Maximum context length to consider while preparing dataset.\n",
    "epoch_metrics = []\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/10k_samples\"   # UPDATE PATH\n",
    "CLEAN_TEXT_COLUMN='article'\n",
    "SUMMARY_COLUMN='highlights'"
   ],
   "id": "initial_id",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set your data path in Google Drive\n",
    "# DRIVE_DATA_PATH = '/content/drive/MyDrive/processed/'  # Update this path"
   ],
   "id": "5f794446e6bc4da9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# Create a local directory for mounting\n",
    "!mkdir results_t5_large\n",
    "\n",
    "# Mount the GCS bucket\n",
    "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
    "!gcsfuse --implicit-dirs t5_large_model results_t5_large"
   ],
   "id": "1bf5a83512308b48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install tqdm\n",
    "!pip install tensorboard-data-server\n",
    "!pip install tbparse"
   ],
   "id": "1738a491dc5fb435"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n"
   ],
   "id": "bf1efda42d7500a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load data from Google Drive\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/val.csv\")\n",
    "# test_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/test.csv\")\n",
    "\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2, shuffle=True)\n",
    "\n",
    "# train_df = train_df.dropna(subset=['Summary', 'clean_text'])\n",
    "# val_df = val_df.dropna(subset=['Summary', 'clean_text'])\n",
    "\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Val:\", len(val_df))\n",
    "# print(\"Test:\", len(test_df))"
   ],
   "id": "cff1e53b6f43aae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "# full_dataset = train_df.train_test_split(test_size=0.2, shuffle=True)\n",
    "# dataset_train = train_df # full_dataset['train']\n",
    "# dataset_valid =  val_df # full_dataset['test']\n",
    "\n",
    "from datasets import Dataset\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_valid = Dataset.from_pandas(val_df)\n",
    "\n",
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ],
   "id": "4b50fac36cabd761"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_longest_length(dataset):\n",
    "    \"\"\"\n",
    "    Find the longest article and summary in the entire training set.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    counter_4k = 0\n",
    "    counter_2k = 0\n",
    "    counter_1k = 0\n",
    "    counter_500 = 0\n",
    "    counter_700 = 0\n",
    "    for text in dataset:\n",
    "        corpus = [\n",
    "            word for word in text.split()\n",
    "        ]\n",
    "        if len(corpus) > 4000:\n",
    "            counter_4k += 1\n",
    "        if len(corpus) > 2000:\n",
    "            counter_2k += 1\n",
    "        if len(corpus) > 1000:\n",
    "            counter_1k += 1\n",
    "        if len(corpus) > 700:\n",
    "            counter_700 += 1\n",
    "        if len(corpus) > 500:\n",
    "            counter_500 += 1\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500\n",
    "\n",
    "longest_article_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500 = find_longest_length(dataset_train[CLEAN_TEXT_COLUMN])\n",
    "print(f\"Longest article length: {longest_article_length} words\")\n",
    "print(f\"Artciles larger than 4000 words: {counter_4k}\")\n",
    "print(f\"Artciles larger than 2000 words: {counter_2k}\")\n",
    "print(f\"Artciles larger than 1000 words: {counter_1k}\")\n",
    "print(f\"Artciles larger than 700 words: {counter_700}\")\n",
    "print(f\"Artciles larger than 500 words: {counter_500}\")\n",
    "longest_summary_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500 = find_longest_length(dataset_train[SUMMARY_COLUMN])\n",
    "print(f\"Longest summary length: {longest_summary_length} words\")\n",
    "print(f\"Summaries larger than 4000 words: {counter_4k}\")\n",
    "print(f\"Summaries larger than 2000 words: {counter_2k}\")\n",
    "print(f\"Summaries larger than 1000 words: {counter_1k}\")\n",
    "print(f\"Summaries larger than 700 words: {counter_700}\")\n",
    "print(f\"Summaries larger than 500 words: {counter_500}\")"
   ],
   "id": "2db5987cad8d824f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_avg_sentence_length(dataset):\n",
    "    \"\"\"\n",
    "    Find the average sentence in the entire training set.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "    for text in dataset:\n",
    "        corpus = [\n",
    "            word for word in text.split()\n",
    "        ]\n",
    "        sentence_lengths.append(len(corpus))\n",
    "    return sum(sentence_lengths)/len(sentence_lengths)\n",
    "\n",
    "avg_article_length = find_avg_sentence_length(dataset_train[CLEAN_TEXT_COLUMN])\n",
    "print(f\"Average article length: {avg_article_length} words\")\n",
    "avg_summary_length = find_avg_sentence_length(dataset_train[SUMMARY_COLUMN])\n",
    "print(f\"Averrage summary length: {avg_summary_length} words\")"
   ],
   "id": "accfe5ab87012b09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL)\n",
    "# Function to convert text data into model inputs and targets\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"summarize: {article}\" for article in examples[CLEAN_TEXT_COLUMN]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    # Set up the tokenizer for targets\n",
    "    targets = [summary for summary in examples[SUMMARY_COLUMN]]\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the function to the whole dataset\n",
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=NUM_PROCS\n",
    ")"
   ],
   "id": "33facb0dc81cff62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(MODEL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ],
   "id": "1df403ad7bc9e9de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak.\n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels"
   ],
   "id": "bbabd166fe85ccae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainerCallback\n",
    "import time\n",
    "writer = SummaryWriter(log_dir=OUT_DIR)\n",
    "\n",
    "class GpuLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            self.writer.add_scalar(\"gpu_memory_gb\", gpu_mem, state.global_step)\n",
    "        return control\n",
    "\n"
   ],
   "id": "f25c6962979f2b05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions[0], eval_pred.label_ids\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=['rouge1','rouge2','rougeL']\n",
    "    )\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # NEW: Log to TensorBoard\n",
    "    for k, v in result.items():\n",
    "        writer.add_scalar(f\"eval/{k}\", v, trainer.state.global_step)\n",
    "\n",
    "    # if trainer.state.is_local_process_zero and trainer.state.epoch is not None:\n",
    "    #     print(result)\n",
    "    #     pd.DataFrame([result]).to_csv(f\"{OUT_DIR}/rouge_epoch_{int(trainer.state.epoch)}.csv\")\n",
    "\n",
    "\n",
    "    # FIXED: make sure it only runs on epoch boundaries\n",
    "    # if trainer.state.is_local_process_zero and trainer.state.epoch is not None:\n",
    "    #     epoch_num = int(trainer.state.epoch)\n",
    "    #     print(f\"[Saving ROUGE metrics for epoch {epoch_num}]\")\n",
    "    #     pd.DataFrame([result]).to_csv(f\"{OUT_DIR}/rouge_epoch_{epoch_num}.csv\", index=False)\n",
    "    pd.DataFrame([result]).to_csv(f\"{OUT_DIR}/rouge_results_step_{trainer.state.global_step}.csv\")\n",
    "\n",
    "\n",
    "    # NEW: Save as CSV for later plotting\n",
    "    # pd.DataFrame([result]).to_csv(f\"{OUT_DIR}/rouge_results_step_{trainer.state.global_step}.csv\")\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ],
   "id": "ed98cb4f44677e03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=2, #todo to test this how much to use more means faster\n",
    "    per_device_eval_batch_size=4,#todo to test this how much to use\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=200,\n",
    "    logging_strategy=\"epoch\",\n",
    "    # eval_strategy='steps',\n",
    "    eval_strategy='epoch',\n",
    "\n",
    "    eval_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    report_to='tensorboard',\n",
    "\n",
    "    learning_rate=0.0001,\n",
    "    dataloader_num_workers=4,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    tf32=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.add_callback(GpuLoggerCallback(writer))\n",
    "pd.DataFrame(trainer.state.log_history).to_csv(f\"{OUT_DIR}/training_history.csv\")\n",
    "\n",
    "start = time.time()\n",
    "history = trainer.train()\n",
    "end = time.time()\n",
    "\n",
    "writer.add_scalar(\"total_training_time_seconds\", end - start, 0)"
   ],
   "id": "ce310833983f2902"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!gsutil cp -r /content/results_t5base/4k_samples gs://models_checkpoint/models/results_t5base/4k_samples\n",
   "id": "a6b552e09a63030b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer.save_pretrained(OUT_DIR)\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "last_ckpt = get_last_checkpoint(OUT_DIR)\n",
    "model_path = last_ckpt if last_ckpt else OUT_DIR\n",
    "\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(OUT_DIR)"
   ],
   "id": "fcc5a32aeac15721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # 1. Tokenize properly (returns attention mask too)\n",
    "    encoded = tokenizer(\n",
    "        \"summarize: \" + text,\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # 2. Move everything to the same device as the model\n",
    "    device = model.device\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    # 3. Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        **encoded,\n",
    "        max_length=128,         # not 50 â†’ 50 is too short for news\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=1.1,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # 4. Decode\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ],
   "id": "3ee2243f63f8cf00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "nli_tok = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(device)\n",
    "\n",
    "def hallucination_rate(summary, source):\n",
    "    sentences = nltk.sent_tokenize(summary)\n",
    "    hallucinated = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = nli_tok.encode_plus(source, sent, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        logits = nli_model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "        contradiction = probs[0].item()\n",
    "        entailment = probs[2].item()\n",
    "\n",
    "        if contradiction > entailment:\n",
    "            hallucinated += 1\n",
    "\n",
    "    return hallucinated / len(sentences)\n"
   ],
   "id": "6785484fb5470848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/test.csv\")\n",
   "id": "f4a3e0c4d7181475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "results = []\n",
    "\n",
    "# Process test set (limit to first 50 for demo)\n",
    "test_limit = 100\n",
    "test_subset = test_df.head(test_limit)\n",
    "\n",
    "print(f\"Processing {len(test_subset)} articles...\")\n",
    "\n",
    "for idx, row in tqdm(test_subset.iterrows(), total=len(test_subset)):\n",
    "    article_id = int(idx)\n",
    "    text = str(row[CLEAN_TEXT_COLUMN])\n",
    "    original_summary = str(row[SUMMARY_COLUMN])\n",
    "\n",
    "    summary = summarize_text(text, model, tokenizer)\n",
    "\n",
    "    results.append({\n",
    "        \"article_id\": article_id,\n",
    "        \"original_text\": text,\n",
    "        \"summary\": summary,\n",
    "        \"original_summary\": original_summary\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(results)} articles\")\n",
    "\n",
    "hallucinations = []\n",
    "for sample in results:\n",
    "    rate = hallucination_rate(sample[\"summary\"], sample[\"original_text\"])\n",
    "    hallucinations.append(rate)\n",
    "\n",
    "avg_hall = np.mean(hallucinations)\n",
    "writer.add_scalar(\"hallucination_rate\", avg_hall, 0)\n",
    "pd.DataFrame({\"hallucination_rate\": hallucinations}).to_csv(f\"{OUT_DIR}/hallucination.csv\")\n",
    "\n"
   ],
   "id": "c911dc27f1f61d52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save results\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "results_dir = '/content/t5_results_new_dataset'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "output_path = f\"{OUT_DIR}/t5_10_results.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")\n",
    "\n",
    "# Optionally save to Google Drive\n",
    "drive_output_path = f\"{DRIVE_DATA_PATH}/../results/t5_2k_results.json\"\n",
    "os.makedirs(os.path.dirname(drive_output_path), exist_ok=True)\n",
    "with open(drive_output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results also saved to Google Drive: {drive_output_path}\")\n"
   ],
   "id": "34cf82cfc900ff92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tbparse import SummaryReader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#/content/results_t5base/4k_samples_test/events.out.tfevents.1764615908.80a126f62e9a.934.3\n",
    "  # change for each run/content/results_t5base/4k_samples/events.out.tfevents.1764612565.80a126f62e9a.934.0\n",
    "\n",
    "reader = SummaryReader(OUT_DIR, pivot=False)\n",
    "df = reader.scalars\n",
    "df.to_csv(f'{OUT_DIR}/metrics.csv')\n",
    "\n",
    "# df_pivoted_cleaned = df.pivot_table(\n",
    "#     index='step',\n",
    "#     columns='tag',\n",
    "#     values='value',\n",
    "# )\n",
    "\n",
    "# df_pivoted_cleaned\n",
    "# df_pivoted = df.pivot(index='tag1', columns='tag')\n",
    "\n",
    "\n",
    "# EVAL_PREFIX = 'eval/'\n",
    "# eval_columns = [col for col in df.columns if col.startswith(EVAL_PREFIX)]\n",
    "\n",
    "# # 2. Filter the DataFrame\n",
    "# # Use .dropna() to remove rows (axis=0) where all of the columns in 'subset' are NaN.\n",
    "# df_filtered = df.dropna(subset=eval_columns, how='all')\n",
    "\n",
    "# # Display the first few rows of the cleaned data\n",
    "# print(df_filtered.head())\n",
    "\n",
    "# print(\"DF columns:\", df.columns)\n",
    "# print(\"Number of rows:\", len(df))\n",
    "# df.head()"
   ],
   "id": "645a4c791ddc6132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $LOG_DIR"
   ],
   "id": "56acdf91c88fe5d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# Create a local directory for mounting\n",
    "# !mkdir results_t5base\n",
    "\n",
    "# Mount the GCS bucket\n",
    "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
    "# !gcsfuse --implicit-dirs models_checkpoint results_t5base\n",
    "\n",
    "!gsutil cp -r /content/results_t5base/2k_samples gs://models_checkpoint/models/results_t5base/2k_samples\n",
    "\n"
   ],
   "id": "afc347b11f6bc1de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!gsutil cp -r /content/results_t5base/2k_samples gs://models_checkpoint/models/results_t5base/2k_samples",
   "id": "9b861ea1c1f7e94b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3067b639b61349b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
