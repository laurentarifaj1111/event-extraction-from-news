{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/50k_samples_new\"\n",
    "CLEAN_TEXT_COLUMN='article'\n",
    "SUMMARY_COLUMN='highlights'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "# This cell mounts your Google Drive to access your dataset files.\n",
    "# You'll be prompted to authorize access - follow the instructions.\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "950d917bbf88aede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -U datasets",
   "id": "183ec0818727f13b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/train.csv\")\n",
    "val_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/val.csv\").head(2000)\n",
    "\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Val:\", len(val_df))"
   ],
   "id": "863656a1b33eae06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_longest_length(dataset):\n",
    "    \"\"\"\n",
    "    Find the longest article and summary in the entire training set.\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    counter_4k = 0\n",
    "    counter_2k = 0\n",
    "    counter_1k = 0\n",
    "    counter_500 = 0\n",
    "    counter_700 = 0\n",
    "    for text in dataset:\n",
    "        corpus = [\n",
    "            word for word in text.split()\n",
    "        ]\n",
    "        if len(corpus) > 4000:\n",
    "            counter_4k += 1\n",
    "        if len(corpus) > 2000:\n",
    "            counter_2k += 1\n",
    "        if len(corpus) > 1000:\n",
    "            counter_1k += 1\n",
    "        if len(corpus) > 700:\n",
    "            counter_700 += 1\n",
    "        if len(corpus) > 500:\n",
    "            counter_500 += 1\n",
    "        if len(corpus) > max_length:\n",
    "            max_length = len(corpus)\n",
    "    return max_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500\n",
    "\n",
    "longest_article_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500 = find_longest_length(dataset_train[CLEAN_TEXT_COLUMN])\n",
    "print(f\"Longest article length: {longest_article_length} words\")\n",
    "print(f\"Artciles larger than 4000 words: {counter_4k}\")\n",
    "print(f\"Artciles larger than 2000 words: {counter_2k}\")\n",
    "print(f\"Artciles larger than 1000 words: {counter_1k}\")\n",
    "print(f\"Artciles larger than 700 words: {counter_700}\")\n",
    "print(f\"Artciles larger than 500 words: {counter_500}\")\n",
    "longest_summary_length, counter_4k, counter_2k, counter_1k, counter_700, counter_500 = find_longest_length(dataset_train[SUMMARY_COLUMN])\n",
    "print(f\"Longest summary length: {longest_summary_length} words\")\n",
    "print(f\"Summaries larger than 4000 words: {counter_4k}\")\n",
    "print(f\"Summaries larger than 2000 words: {counter_2k}\")\n",
    "print(f\"Summaries larger than 1000 words: {counter_1k}\")\n",
    "print(f\"Summaries larger than 700 words: {counter_700}\")\n",
    "print(f\"Summaries larger than 500 words: {counter_500}\")"
   ],
   "id": "813c954579b616d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_avg_sentence_length(dataset):\n",
    "    \"\"\"\n",
    "    Find the average sentence in the entire training set.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "    for text in dataset:\n",
    "        corpus = [\n",
    "            word for word in text.split()\n",
    "        ]\n",
    "        sentence_lengths.append(len(corpus))\n",
    "    return sum(sentence_lengths)/len(sentence_lengths)\n",
    "\n",
    "avg_article_length = find_avg_sentence_length(dataset_train[CLEAN_TEXT_COLUMN])\n",
    "print(f\"Average article length: {avg_article_length} words\")\n",
    "avg_summary_length = find_avg_sentence_length(dataset_train[SUMMARY_COLUMN])\n",
    "print(f\"Averrage summary length: {avg_summary_length} words\")"
   ],
   "id": "9e4e55e7a9884332"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
