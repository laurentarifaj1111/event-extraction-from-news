{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "CLEAN_TEXT_COLUMN='article'\n",
    "SUMMARY_COLUMN='highlights'"
   ],
   "metadata": {
    "id": "RctCKn0a2Mgt"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tensorboard\n",
    "!pip install tensorboard-data-server\n",
    "!pip install google-cloud-storage\n",
    "!pip install tbparse matplotlib seaborn pandas numpy\n",
    "\n"
   ],
   "metadata": {
    "id": "FHGdehOv2W9v"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# GOOGLE CLOUD STORAGE (GCS) SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# This sets up gcsfuse to mount a Google Cloud Storage bucket for model storage.\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n"
   ],
   "metadata": {
    "id": "ZwWQz0HAURn0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "806f8dd9-27a2-400f-989a-94ebce5c90e9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir pegasus_model\n",
    "!gcsfuse --implicit-dirs pegasus_2k_model pegasus_model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHLkVuZe_YKX",
    "outputId": "56409f9c-d4d4-4278-e949-c3be1eaeb7ac"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "## THIS IS TO LOAD ALL MODELS AND ON A100\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "import time\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    PegasusTokenizer,\n",
    "    PegasusForConditionalGeneration,\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# A100 OPTIMIZATIONS\n",
    "# ==================================================\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = \"cuda\"\n",
    "assert torch.cuda.is_available(), \"GPU is required\"\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ==================================================\n",
    "# MODEL CONFIG\n",
    "# ==================================================\n",
    "MODEL_CONFIG = {\n",
    "    \"T5 Base\": {\n",
    "        \"type\": \"t5\",\n",
    "        \"base_model\": \"t5-base\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/t5-base/2k/checkpoint-XXX\",\n",
    "            \"10k\": \"/content/models/t5-base/10k/checkpoint-XXX\",\n",
    "            \"50k\": \"/content/models/t5-base/50k/checkpoint-XXX\",\n",
    "        }\n",
    "    },\n",
    "    \"T5 Large\": {\n",
    "        \"type\": \"t5\",\n",
    "        \"base_model\": \"t5-large\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/t5-large/2k/checkpoint-XXX\",\n",
    "            \"10k\": \"/content/models/t5-large/10k/checkpoint-XXX\",\n",
    "            \"50k\": \"/content/models/t5-large/50k/checkpoint-XXX\",\n",
    "        }\n",
    "    },\n",
    "    \"Pegasus Base\": {\n",
    "        \"type\": \"pegasus\",\n",
    "        \"base_model\": \"google/pegasus-arxiv\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/pegasus-base/2k/checkpoint-XXX\",\n",
    "            \"10k\": \"/content/models/pegasus-base/10k/checkpoint-XXX\",\n",
    "            \"50k\": \"/content/models/pegasus-base/50k/checkpoint-XXX\",\n",
    "        }\n",
    "    },\n",
    "    \"Pegasus CNN\": {\n",
    "        \"type\": \"pegasus\",\n",
    "        \"base_model\": \"google/pegasus-cnn_dailymail\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/pegasus-cnn/2k/checkpoint-XXX\",\n",
    "            \"10k\": \"/content/models/pegasus-cnn/10k/checkpoint-XXX\",\n",
    "            \"50k\": \"/content/models/pegasus-cnn/50k/checkpoint-XXX\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# ==================================================\n",
    "# LOAD MODELS (FP16)\n",
    "# ==================================================\n",
    "loaded_models = {}\n",
    "loaded_tokenizers = {}\n",
    "\n",
    "print(\"Loading models (FP16)...\")\n",
    "\n",
    "for model_name, cfg in MODEL_CONFIG.items():\n",
    "    for variant, path in cfg[\"variants\"].items():\n",
    "        key = f\"{model_name} ({variant})\"\n",
    "        print(f\"\\nLoading {key}\")\n",
    "        start = time.time()\n",
    "\n",
    "        if cfg[\"type\"] == \"t5\":\n",
    "            tokenizer = T5Tokenizer.from_pretrained(cfg[\"base_model\"])\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                path,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = PegasusTokenizer.from_pretrained(cfg[\"base_model\"])\n",
    "            model = PegasusForConditionalGeneration.from_pretrained(\n",
    "                path,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        loaded_models[key] = model\n",
    "        loaded_tokenizers[key] = tokenizer\n",
    "\n",
    "        print(f\"{key} loaded in {time.time() - start:.2f}s\")\n",
    "\n",
    "print(\"\\nâœ… All models loaded in FP16\")\n",
    "\n",
    "# ==================================================\n",
    "# SUMMARIZATION (FAST PATH)\n",
    "# ==================================================\n",
    "def summarize(text, model_key):\n",
    "    model = loaded_models[model_key]\n",
    "    tokenizer = loaded_tokenizers[model_key]\n",
    "\n",
    "    if model_key.startswith(\"T5\"):\n",
    "        text = \"summarize: \" + text\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=128,\n",
    "            num_beams=4,          # good quality/speed balance\n",
    "            early_stopping=True,\n",
    "            use_cache=True        # decoder cache (important)\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ==================================================\n",
    "# UI\n",
    "# ==================================================\n",
    "MODEL_CHOICES = list(loaded_models.keys())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## ðŸš€ A100-Optimized Multi-Model Summarization\")\n",
    "\n",
    "    model_selector = gr.Dropdown(\n",
    "        choices=MODEL_CHOICES,\n",
    "        value=MODEL_CHOICES[0],\n",
    "        label=\"Select Model Variant\"\n",
    "    )\n",
    "\n",
    "    input_text = gr.Textbox(\n",
    "        lines=10,\n",
    "        label=\"Input text\"\n",
    "    )\n",
    "\n",
    "    summarize_btn = gr.Button(\"Summarize\")\n",
    "\n",
    "    output_text = gr.Textbox(\n",
    "        lines=5,\n",
    "        label=\"Summary\"\n",
    "    )\n",
    "\n",
    "    summarize_btn.click(\n",
    "        fn=summarize,\n",
    "        inputs=[input_text, model_selector],\n",
    "        outputs=output_text\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ],
   "metadata": {
    "id": "QiLObDgNsLJ0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Imports\n",
    "# =============================================================\n",
    "import gradio as gr\n",
    "import torch\n",
    "import time\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    PegasusTokenizer,\n",
    "    PegasusForConditionalGeneration,\n",
    ")\n",
    "\n",
    "# =============================================================\n",
    "# GPU & A100-specific optimizations\n",
    "# =============================================================\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "assert torch.cuda.is_available(), \"GPU is required for this notebook\"\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Model configuration\n",
    "# Defines model families and fine-tuned variants\n",
    "# =============================================================\n",
    "MODEL_CONFIG = {\n",
    "    \"T5 Base\": {\n",
    "        \"type\": \"t5\",\n",
    "        \"base_model\": \"t5-base\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/t5_base_2k\",\n",
    "            \"10k\": \"/content/models/t5_base_10k\",\n",
    "            \"50k\": \"/content/models/t5_base_50k\",\n",
    "        },\n",
    "    },\n",
    "    \"T5 Large\": {\n",
    "        \"type\": \"t5\",\n",
    "        \"base_model\": \"t5-large\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/t5_large_2k\",\n",
    "            \"10k\": \"/content/models/t5_large_10k\",\n",
    "            \"50k\": \"/content/models/t5_large_50k\",\n",
    "        },\n",
    "    },\n",
    "    \"Pegasus Base\": {\n",
    "        \"type\": \"pegasus\",\n",
    "        \"base_model\": \"google/pegasus-arxiv\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/pegasus_base_2k\",\n",
    "            \"10k\": \"/content/models/pegasus_base_10k\",\n",
    "            \"50k\": \"/content/models/pegasus_base_50k\",\n",
    "        },\n",
    "    },\n",
    "    \"Pegasus CNN\": {\n",
    "        \"type\": \"pegasus\",\n",
    "        \"base_model\": \"google/pegasus-cnn_dailymail\",\n",
    "        \"variants\": {\n",
    "            \"2k\":  \"/content/models/pegasus_cnn_daily_mail_2k\",\n",
    "            \"10k\": \"/content/models/pegasus_cnn_daily_mail_10k\",\n",
    "            \"50k\": \"/content/models/pegasus_cnn_daily_mail_50k\",\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Load models and tokenizers (FP16, inference mode)\n",
    "# =============================================================\n",
    "loaded_models = {}\n",
    "loaded_tokenizers = {}\n",
    "\n",
    "print(\"Loading models in FP16 mode...\")\n",
    "\n",
    "for model_name, cfg in MODEL_CONFIG.items():\n",
    "    for variant, checkpoint_path in cfg[\"variants\"].items():\n",
    "        model_key = f\"{model_name} ({variant})\"\n",
    "        print(f\"\\nLoading {model_key}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if cfg[\"type\"] == \"t5\":\n",
    "            tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\n",
    "                checkpoint_path,\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = PegasusTokenizer.from_pretrained(checkpoint_path)\n",
    "            model = PegasusForConditionalGeneration.from_pretrained(\n",
    "                checkpoint_path,\n",
    "                torch_dtype=torch.float16,\n",
    "            )\n",
    "\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        loaded_models[model_key] = model\n",
    "        loaded_tokenizers[model_key] = tokenizer\n",
    "\n",
    "        print(f\"{model_key} loaded in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "print(\"\\nâœ… All models loaded successfully\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Fast summarization function (inference-only)\n",
    "# =============================================================\n",
    "def summarize(text, model_key):\n",
    "    \"\"\"\n",
    "    Generate a summary using the selected model variant.\n",
    "    Optimized for low-latency inference on A100.\n",
    "    \"\"\"\n",
    "\n",
    "    model = loaded_models[model_key]\n",
    "    tokenizer = loaded_tokenizers[model_key]\n",
    "\n",
    "    # T5 requires a task prefix\n",
    "    if model_key.startswith(\"T5\"):\n",
    "        text = \"summarize: \" + text\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        summary_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Gradio UI\n",
    "# =============================================================\n",
    "MODEL_CHOICES = list(loaded_models.keys())\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## A100-Optimized Multi-Model News Summarization\")\n",
    "\n",
    "    model_selector = gr.Dropdown(\n",
    "        choices=MODEL_CHOICES,\n",
    "        value=MODEL_CHOICES[0],\n",
    "        label=\"Select Model Variant\",\n",
    "    )\n",
    "\n",
    "    input_text = gr.Textbox(\n",
    "        lines=10,\n",
    "        label=\"Input Text\",\n",
    "    )\n",
    "\n",
    "    summarize_button = gr.Button(\"Summarize\")\n",
    "\n",
    "    output_text = gr.Textbox(\n",
    "        lines=5,\n",
    "        label=\"Generated Summary\",\n",
    "    )\n",
    "\n",
    "    summarize_button.click(\n",
    "        fn=summarize,\n",
    "        inputs=[input_text, model_selector],\n",
    "        outputs=output_text,\n",
    "    )\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================\n",
    "# Launch Gradio app\n",
    "# =============================================================\n",
    "demo.launch(share=True)\n"
   ]
  }
 ]
}
