{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1765594364498,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "EMIflcpRwSqn"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PEGASUS CNN/DAILYMAIL MODEL FINE-TUNING CONFIGURATION\n",
    "# ============================================================================\n",
    "# This notebook fine-tunes a Pegasus model pre-trained on CNN/DailyMail dataset.\n",
    "# The CNN/DailyMail variant is optimized for news article summarization.\n",
    "#\n",
    "# Configuration Parameters:\n",
    "# - MODEL: Pre-trained Pegasus model (google/pegasus-cnn_dailymail)\n",
    "# - OUT_DIR = # Directory to save model checkpoints and results\n",
    "# - DRIVE_DATA_PATH: Path to dataset in Google Drive (UPDATE THIS!)\n",
    "# - CLEAN_TEXT_COLUMN: Column name in CSV containing article text\n",
    "# - SUMMARY_COLUMN: Column name in CSV containing reference summaries\n",
    "# ============================================================================\n",
    "\n",
    "MODEL = 'google/pegasus-cnn_dailymail'  \n",
    "OUT_DIR = 'results-pegasus/2k_samples'  \n",
    "\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/10k_samples\"\n",
    "\n",
    "# Column names in your CSV file\n",
    "CLEAN_TEXT_COLUMN = 'article'  # Column containing the article text\n",
    "SUMMARY_COLUMN = 'highlights'  # Column containing the reference summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1465,
     "status": "ok",
     "timestamp": 1765594367574,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "DubrCb8GwXyg",
    "outputId": "5ff9e8ff-3bf6-4db2-9f62-99250c32bd41"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MOUNT GOOGLE DRIVE\n",
    "# ============================================================================\n",
    "# This cell mounts your Google Drive to access your dataset files.\n",
    "# You'll be prompted to authorize access - follow the instructions.\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34614,
     "status": "ok",
     "timestamp": 1765594432887,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "eHjT8Kkrn3CH",
    "outputId": "8ab27aeb-1404-48ae-a4ad-390c21fd873c"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: GOOGLE CLOUD STORAGE (GCS) SETUP\n",
    "# ============================================================================\n",
    "# This cell is optional - only needed if you want to save/load models from GCS.\n",
    "# If you're only using Google Drive, you can skip this cell.\n",
    "#\n",
    "# This sets up gcsfuse to mount a Google Cloud Storage bucket for model storage.\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse (Google Cloud Storage FUSE - allows mounting GCS buckets as filesystem)\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "# Create a local directory for mounting the GCS bucket\n",
    "!mkdir -p results-pegasus\n",
    "\n",
    "# Mount the GCS bucket\n",
    "!gcsfuse --implicit-dirs pegasus_cnn_daily_mail_2k results-pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40191,
     "status": "ok",
     "timestamp": 1765592972299,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "mWoALIEfzkzm",
    "outputId": "a14b6e0e-1d38-44e7-d995-b9d5365698a8"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12355,
     "status": "ok",
     "timestamp": 1765594455382,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "89nzRnGjz3CL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration,\n",
    "    PegasusTokenizer,EarlyStoppingCallback,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1765594455396,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "4NGDTnQ1wuHZ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINE-TUNING SETUP FUNCTION\n",
    "# ============================================================================\n",
    "# This function prepares the Pegasus model and training configuration.\n",
    "# It handles model loading, training arguments setup, and trainer creation.\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "  \"\"\"\n",
    "  Prepare configurations and base model for fine-tuning.\n",
    "  \n",
    "  Args:\n",
    "    model_name: Name of the pre-trained Pegasus model\n",
    "    tokenizer: Pegasus tokenizer instance\n",
    "    train_dataset: Training dataset\n",
    "    val_dataset: Validation dataset (optional)\n",
    "    freeze_encoder: If True, freeze encoder parameters (transfer learning)\n",
    "    output_dir: Directory to save checkpoints\n",
    "    \n",
    "  Returns:\n",
    "    Trainer: Configured Seq2SeqTrainer instance\n",
    "  \"\"\"\n",
    "  # Determine device (GPU if available, otherwise CPU)\n",
    "  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  print(f\"Using device: {torch_device}\")\n",
    "  \n",
    "  # Load pre-trained Pegasus model\n",
    "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "  if freeze_encoder:\n",
    "    for param in model.model.encoder.parameters():\n",
    "      param.requires_grad = False\n",
    "    print(\"Encoder frozen - only decoder will be trained\")\n",
    "\n",
    "\n",
    "  # Training configuration\n",
    "  training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR,  # Where to save checkpoints\n",
    "    num_train_epochs=10,  # Total number of training epochs\n",
    "    \n",
    "    # Batch sizes (adjust based on GPU memory)\n",
    "    per_device_train_batch_size=8,  # Training batch size per GPU\n",
    "    per_device_eval_batch_size=16,  # Evaluation batch size \n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy='epoch',  # Save after each epoch\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=0.0001,  # Learning rate\n",
    "    warmup_steps=0.1,  # Warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients (effective batch size = 8 * 8 = 64)\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_dir=f'{OUT_DIR}/logs',  # TensorBoard log directory\n",
    "    logging_strategy=\"epoch\",  # Also log at end of each epoch\n",
    "    eval_strategy='epoch',  # Evaluate after each epoch\n",
    "    report_to='tensorboard',  # Log to TensorBoard\n",
    "    \n",
    "    # Generation settings (for evaluation)\n",
    "    predict_with_generate=True,  # Use generation during evaluation\n",
    "    \n",
    "    # Performance optimizations (for modern GPUs)\n",
    "    bf16=True,  # Use bfloat16 precision (faster, less memory)\n",
    "    fp16=False,  # Don't use float16 (bf16 is better for training)\n",
    "    tf32=True,  # Use TensorFloat-32 (faster on A100 GPUs)\n",
    "    torch_compile=True,  # Compile model for faster execution\n",
    "    \n",
    "    # Model selection\n",
    "    metric_for_best_model='eval_loss',  # Use validation loss to select best model\n",
    "    greater_is_better=False,  # Lower loss is better\n",
    "    load_best_model_at_end=True  # Load best model after training\n",
    "  )\n",
    "  \n",
    "  # Create trainer with validation dataset\n",
    "  trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,  # Validation dataset for evaluation\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,  # ROUGE metrics computation\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement for 3 epochs\n",
    "  )\n",
    "\n",
    "\n",
    "  return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2976,
     "status": "ok",
     "timestamp": 1765594458373,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "--QyWKjbweCG"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TENSORBOARD LOGGING, DATASET CLASSES, AND UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "# This cell sets up TensorBoard logging, custom dataset class, GPU monitoring,\n",
    "# data preparation functions, and evaluation metrics computation.\n",
    "# ============================================================================\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "# Initialize TensorBoard writer (logs will be saved to OUT_DIR)\n",
    "writer = SummaryWriter(log_dir=OUT_DIR)\n",
    "print(f\"TensorBoard logs will be saved to: {OUT_DIR}\")\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset class for Pegasus model training.\n",
    "    \n",
    "    This class wraps tokenized encodings and labels into a format\n",
    "    that PyTorch DataLoader can use for efficient batching.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encodings: Tokenized input texts (dictionary with 'input_ids', 'attention_mask')\n",
    "            labels: Tokenized target summaries (dictionary with 'input_ids')\n",
    "        \"\"\"\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single training example.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with input_ids, attention_mask, and labels as tensors\n",
    "        \"\"\"\n",
    "        # Convert encodings to tensors\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Add labels (target summaries) as tensors\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of examples in the dataset.\"\"\"\n",
    "        return len(self.labels['input_ids'])\n",
    "\n",
    "class GpuLoggerCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    Custom callback to log GPU memory usage during training.\n",
    "    \n",
    "    This helps monitor if you're running out of GPU memory and need to\n",
    "    reduce batch size or other memory-intensive settings.\n",
    "    \"\"\"\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        \"\"\"\n",
    "        Called after each training step.\n",
    "        Logs current GPU memory usage to TensorBoard.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            # Get current GPU memory usage in GB\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            # Log to TensorBoard\n",
    "            self.writer.add_scalar(\"gpu_memory_gb\", gpu_mem, state.global_step)\n",
    "        return control\n",
    "\n",
    "def prepare_data(model_name,\n",
    "                 train_texts, train_labels,\n",
    "                 val_texts=None, val_labels=None,\n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning.\n",
    "  \n",
    "  This function tokenizes articles and summaries, creating datasets\n",
    "  ready for training.\n",
    "  \n",
    "  Args:\n",
    "    model_name: Name of the Pegasus model (for tokenizer)\n",
    "    train_texts: List of training article texts\n",
    "    train_labels: List of training summary texts\n",
    "    val_texts: List of validation article texts (optional)\n",
    "    val_labels: List of validation summary texts (optional)\n",
    "    test_texts: List of test article texts (optional)\n",
    "    test_labels: List of test summary texts (optional)\n",
    "    \n",
    "  Returns:\n",
    "    Tuple of (train_dataset, val_dataset, test_dataset, tokenizer)\n",
    "  \"\"\"\n",
    "  # Load Pegasus tokenizer\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  # Check if validation and test sets are provided\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    \"\"\"\n",
    "    Tokenize articles and summaries.\n",
    "    \n",
    "    Args:\n",
    "      texts: List of article texts\n",
    "      labels: List of summary texts\n",
    "      \n",
    "    Returns:\n",
    "      PegasusDataset instance with tokenized data\n",
    "    \"\"\"\n",
    "    # Tokenize inputs (articles)\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    # Tokenize targets (summaries)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    # Create dataset\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  # Tokenize all datasets\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute ROUGE metrics for model evaluation.\n",
    "    \n",
    "    This function is called automatically during validation.\n",
    "    It decodes predictions and references, then computes ROUGE scores.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Predictions and labels from the model\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metric scores (ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-Lsum, gen_len)\n",
    "    \"\"\"\n",
    "    # Load ROUGE metric\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    \n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Decode predictions (convert token IDs back to text)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 (ignored tokens) with pad token before decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,  # Use stemming for better matching\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]  # Compute these ROUGE variants\n",
    "    )\n",
    "    \n",
    "    # Calculate average generated summary length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Log metrics to TensorBoard for visualization\n",
    "    for k, v in rouge_result.items():\n",
    "        writer.add_scalar(f\"eval/{k}\", v, trainer.state.global_step)\n",
    "\n",
    "    # Return metrics dictionary\n",
    "    return {k: v for k, v in rouge_result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "executionInfo": {
     "elapsed": 10190417,
     "status": "ok",
     "timestamp": 1765604652939,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     },
     "user_tz": -60
    },
    "id": "SvN3sG6Ixhio",
    "outputId": "8ee521df-8b95-4fe4-d9cd-e4fde4e8d244"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA AND START TRAINING\n",
    "# ============================================================================\n",
    "# Load training and validation datasets from Google Drive, prepare them for training,\n",
    "# and start the fine-tuning process.\n",
    "# ============================================================================\n",
    "\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATA FROM GOOGLE DRIVE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Data path: {DRIVE_DATA_PATH}\")\n",
    "\n",
    "# Load training and validation datasets\n",
    "# Using .head() to limit dataset size for faster training/testing\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/train.csv\").head(2000)  # Limit to 2000 samples\n",
    "print(\"Loading validation data...\")\n",
    "val_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/val.csv\").head(400)  # Limit to 400 samples\n",
    "\n",
    "# Convert pandas DataFrames to HuggingFace Dataset format\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_valid = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Extract article texts and summaries\n",
    "train_texts, train_labels = dataset_train[CLEAN_TEXT_COLUMN], dataset_train[SUMMARY_COLUMN]\n",
    "valid_texts, valid_labels = dataset_valid[CLEAN_TEXT_COLUMN], dataset_valid[SUMMARY_COLUMN]\n",
    "\n",
    "print(f\"\\n✓ Training samples: {len(train_texts)}\")\n",
    "print(f\"✓ Validation samples: {len(valid_texts)}\")\n",
    "\n",
    "# Prepare datasets (tokenize articles and summaries)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARING DATA FOR TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "train_dataset, val_dataset, test_dataset, tokenizer = prepare_data(\n",
    "    MODEL, train_texts, train_labels, valid_texts, valid_labels\n",
    ")\n",
    "print(\"✓ Data tokenized and ready for training\")\n",
    "\n",
    "# Create trainer with fine-tuning configuration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INITIALIZING TRAINER\")\n",
    "print(\"=\" * 60)\n",
    "trainer = prepare_fine_tuning(MODEL, tokenizer, train_dataset, val_dataset=val_dataset)\n",
    "\n",
    "# Add GPU memory monitoring callback\n",
    "trainer.add_callback(GpuLoggerCallback(writer))\n",
    "\n",
    "# Start training!\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training for {EPOCHS} epochs...\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Output directory: {OUT_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  Total time: {training_time / 60:.2f} minutes ({training_time / 3600:.2f} hours)\")\n",
    "\n",
    "# Log total training time to TensorBoard\n",
    "writer.add_scalar(\"total_training_time_seconds\", training_time, 0)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNSuFzF092bQkiHLxKHY6sZ",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
