{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9bcefc0698655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "execution_count": null,
   "id": "79a67fa9db9d7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 0,\n",
    " \"metadata\": {\n",
    "  \"colab\": {\n",
    "   \"provenance\": [],\n",
    "   \"gpuType\": \"A100\",\n",
    "   \"authorship_tag\": \"ABX9TyNSuFzF092bQkiHLxKHY6sZ\"\n",
    "  },\n",
    "  \"kernelspec\": {\n",
    "   \"name\": \"python3\",\n",
    "   \"display_name\": \"Python 3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\"\n",
    "  },\n",
    "  \"accelerator\": \"GPU\"\n",
    " },\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"EMIflcpRwSqn\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594364498,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 7,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"MODEL = 'google/pegasus-cnn_dailymail'\\n\",\n",
    "    \"# MODEL = 'google/pegasus-cnn_dailymail'\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"BATCH_SIZE = 4\\n\",\n",
    "    \"NUM_PROCS = 4\\n\",\n",
    "    \"EPOCHS = 10\\n\",\n",
    "    \"OUT_DIR = 'results-pegasus/2k_samples'\\n\",\n",
    "    \"MAX_LENGTH = 1024 # Maximum context length to consider while preparing dataset.\\n\",\n",
    "    \"epoch_metrics = []\\n\",\n",
    "    \"DRIVE_DATA_PATH = \\\"/content/drive/MyDrive/processed/10k_samples\\\"   # UPDATE PATH\\n\",\n",
    "    \"CLEAN_TEXT_COLUMN='article'\\n\",\n",
    "    \"SUMMARY_COLUMN='highlights'\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from google.colab import drive\\n\",\n",
    "    \"drive.mount('/content/drive')\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"DubrCb8GwXyg\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594367574,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 1465,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    },\n",
    "    \"outputId\": \"5ff9e8ff-3bf6-4db2-9f62-99250c32bd41\"\n",
    "   },\n",
    "   \"execution_count\": 2,\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stdout\",\n",
    "     \"text\": [\n",
    "      \"Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\\\"/content/drive\\\", force_remount=True).\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from google.colab import auth\\n\",\n",
    "    \"auth.authenticate_user()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Install gcsfuse\\n\",\n",
    "    \"!echo \\\"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\\\" > /etc/apt/sources.list.d/gcsfuse.list\\n\",\n",
    "    \"!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\\n\",\n",
    "    \"!apt -qq update\\n\",\n",
    "    \"!apt -qq install gcsfuse\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a local directory for mounting\\n\",\n",
    "    \"!mkdir results-pegasus\\n\",\n",
    "    \"# Mount the GCS bucket\\n\",\n",
    "    \"# Replace 'your-bucket-name' with the actual name of your GCS bucket\\n\",\n",
    "    \"!gcsfuse --implicit-dirs pegasus_cnn_daily_mail_2k results-pegasus\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"id\": \"eHjT8Kkrn3CH\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594432887,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 34614,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    },\n",
    "    \"outputId\": \"8ab27aeb-1404-48ae-a4ad-390c21fd873c\"\n",
    "   },\n",
    "   \"execution_count\": 4,\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stdout\",\n",
    "     \"text\": [\n",
    "      \"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\n\",\n",
    "      \"                                 Dload  Upload   Total   Spent    Left  Speed\\n\",\n",
    "      \"\\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\\r100  1022  100  1022    0     0  12791      0 --:--:-- --:--:-- --:--:-- 12936\\n\",\n",
    "      \"Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\\n\",\n",
    "      \"OK\\n\",\n",
    "      \"48 packages can be upgraded. Run 'apt list --upgradable' to see them.\\n\",\n",
    "      \"\\u001B[1;33mW: \\u001B[0mhttp://packages.cloud.google.com/apt/dists/gcsfuse-bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\\u001B[0m\\n\",\n",
    "      \"\\u001B[1;33mW: \\u001B[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\\u001B[0m\\n\",\n",
    "      \"The following NEW packages will be installed:\\n\",\n",
    "      \"  gcsfuse\\n\",\n",
    "      \"0 upgraded, 1 newly installed, 0 to remove and 48 not upgraded.\\n\",\n",
    "      \"Need to get 15.1 MB of archives.\\n\",\n",
    "      \"After this operation, 0 B of additional disk space will be used.\\n\",\n",
    "      \"Selecting previously unselected package gcsfuse.\\n\",\n",
    "      \"(Reading database ... 121689 files and directories currently installed.)\\n\",\n",
    "      \"Preparing to unpack .../gcsfuse_3.5.4_amd64.deb ...\\n\",\n",
    "      \"Unpacking gcsfuse (3.5.4) ...\\n\",\n",
    "      \"Setting up gcsfuse (3.5.4) ...\\n\",\n",
    "      \"{\\\"timestamp\\\":{\\\"seconds\\\":1765594428,\\\"nanos\\\":441790712},\\\"severity\\\":\\\"INFO\\\",\\\"message\\\":\\\"Start gcsfuse/3.5.4 (Go version go1.24.11) for app \\\\\\\"\\\\\\\" using mount point: /content/results-pegasus\\\\n\\\",\\\"mount-id\\\":\\\"pegasus_cnn_daily_mail_2k-55dafd95\\\"}\\n\",\n",
    "      \"{\\\"timestamp\\\":{\\\"seconds\\\":1765594428,\\\"nanos\\\":441816615},\\\"severity\\\":\\\"INFO\\\",\\\"message\\\":\\\"GCSFuse Config\\\",\\\"mount-id\\\":\\\"pegasus_cnn_daily_mail_2k-55dafd95\\\",\\\"CLI Flags\\\":{\\\"implicit-dirs\\\":\\\"true\\\"}}\\n\",\n",
    "      \"{\\\"timestamp\\\":{\\\"seconds\\\":1765594428,\\\"nanos\\\":441836489},\\\"severity\\\":\\\"INFO\\\",\\\"message\\\":\\\"GCSFuse Config\\\",\\\"mount-id\\\":\\\"pegasus_cnn_daily_mail_2k-55dafd95\\\",\\\"Full Config\\\":{\\\"AppName\\\":\\\"\\\",\\\"CacheDir\\\":\\\"\\\",\\\"CloudProfiler\\\":{\\\"AllocatedHeap\\\":true,\\\"Cpu\\\":true,\\\"Enabled\\\":false,\\\"Goroutines\\\":false,\\\"Heap\\\":true,\\\"Label\\\":\\\"gcsfuse-0.0.0\\\",\\\"Mutex\\\":false},\\\"Debug\\\":{\\\"ExitOnInvariantViolation\\\":false,\\\"Fuse\\\":false,\\\"Gcs\\\":false,\\\"LogMutex\\\":false},\\\"DisableAutoconfig\\\":false,\\\"EnableAtomicRenameObject\\\":true,\\\"EnableGoogleLibAuth\\\":true,\\\"EnableHns\\\":true,\\\"EnableNewReader\\\":true,\\\"EnableUnsupportedPathSupport\\\":false,\\\"FileCache\\\":{\\\"CacheFileForRangeRead\\\":false,\\\"DownloadChunkSizeMb\\\":200,\\\"EnableCrc\\\":false,\\\"EnableODirect\\\":false,\\\"EnableParallelDownloads\\\":false,\\\"ExcludeRegex\\\":\\\"\\\",\\\"ExperimentalParallelDownloadsDefaultOn\\\":true,\\\"IncludeRegex\\\":\\\"\\\",\\\"MaxParallelDownloads\\\":24,\\\"MaxSizeMb\\\":-1,\\\"ParallelDownloadsPerFile\\\":16,\\\"WriteBufferSize\\\":4194304},\\\"FileSystem\\\":{\\\"DirMode\\\":\\\"755\\\",\\\"DisableParallelDirops\\\":false,\\\"ExperimentalEnableDentryCache\\\":false,\\\"ExperimentalEnableReaddirplus\\\":false,\\\"FileMode\\\":\\\"644\\\",\\\"FuseOptions\\\":[],\\\"Gid\\\":-1,\\\"IgnoreInterrupts\\\":true,\\\"KernelListCacheTtlSecs\\\":0,\\\"MaxReadAheadKb\\\":0,\\\"ODirect\\\":false,\\\"PreconditionErrors\\\":true,\\\"RenameDirLimit\\\":0,\\\"TempDir\\\":\\\"\\\",\\\"Uid\\\":-1},\\\"Foreground\\\":false,\\\"GcsAuth\\\":{\\\"AnonymousAccess\\\":false,\\\"KeyFile\\\":\\\"\\\",\\\"ReuseTokenFromUrl\\\":true,\\\"TokenUrl\\\":\\\"\\\"},\\\"GcsConnection\\\":{\\\"BillingProject\\\":\\\"\\\",\\\"ClientProtocol\\\":\\\"http1\\\",\\\"CustomEndpoint\\\":\\\"\\\",\\\"EnableHttpDnsCache\\\":true,\\\"ExperimentalEnableJsonRead\\\":false,\\\"ExperimentalLocalSocketAddress\\\":\\\"\\\",\\\"GrpcConnPoolSize\\\":1,\\\"HttpClientTimeout\\\":0,\\\"LimitBytesPerSec\\\":-1,\\\"LimitOpsPerSec\\\":-1,\\\"MaxConnsPerHost\\\":0,\\\"MaxIdleConnsPerHost\\\":100,\\\"SequentialReadSizeMb\\\":200},\\\"GcsRetries\\\":{\\\"ChunkTransferTimeoutSecs\\\":10,\\\"MaxRetryAttempts\\\":0,\\\"MaxRetrySleep\\\":30000000000,\\\"Multiplier\\\":2,\\\"ReadStall\\\":{\\\"Enable\\\":true,\\\"InitialReqTimeout\\\":20000000000,\\\"MaxReqTimeout\\\":1200000000000,\\\"MinReqTimeout\\\":1500000000,\\\"ReqIncreaseRate\\\":15,\\\"ReqTargetPercentile\\\":0.99}},\\\"ImplicitDirs\\\":true,\\\"List\\\":{\\\"EnableEmptyManagedFolders\\\":false},\\\"Logging\\\":{\\\"FilePath\\\":\\\"\\\",\\\"Format\\\":\\\"json\\\",\\\"LogRotate\\\":{\\\"BackupFileCount\\\":10,\\\"Compress\\\":true,\\\"MaxFileSizeMb\\\":512},\\\"Severity\\\":\\\"INFO\\\"},\\\"MachineType\\\":\\\"\\\",\\\"MetadataCache\\\":{\\\"DeprecatedStatCacheCapacity\\\":20460,\\\"DeprecatedStatCacheTtl\\\":60000000000,\\\"DeprecatedTypeCacheTtl\\\":60000000000,\\\"EnableNonexistentTypeCache\\\":false,\\\"ExperimentalMetadataPrefetchOnMount\\\":\\\"disabled\\\",\\\"NegativeTtlSecs\\\":5,\\\"StatCacheMaxSizeMb\\\":33,\\\"TtlSecs\\\":60,\\\"TypeCacheMaxSizeMb\\\":4},\\\"Metrics\\\":{\\\"BufferSize\\\":256,\\\"CloudMetricsExportIntervalSecs\\\":0,\\\"PrometheusPort\\\":0,\\\"StackdriverExportInterval\\\":0,\\\"UseNewNames\\\":false,\\\"Workers\\\":3},\\\"Monitoring\\\":{\\\"ExperimentalTracingMode\\\":\\\"\\\",\\\"ExperimentalTracingProjectId\\\":\\\"\\\",\\\"ExperimentalTracingSamplingRatio\\\":0},\\\"OnlyDir\\\":\\\"\\\",\\\"Profile\\\":\\\"\\\",\\\"Read\\\":{\\\"BlockSizeMb\\\":16,\\\"EnableBufferedRead\\\":false,\\\"GlobalMaxBlocks\\\":40,\\\"InactiveStreamTimeout\\\":10000000000,\\\"MaxBlocksPerHandle\\\":20,\\\"MinBlocksPerHandle\\\":4,\\\"RandomSeekThreshold\\\":3,\\\"StartBlocksPerHandle\\\":1},\\\"WorkloadInsight\\\":{\\\"ForwardMergeThresholdMb\\\":0,\\\"OutputFile\\\":\\\"\\\",\\\"Visualize\\\":false},\\\"Write\\\":{\\\"BlockSizeMb\\\":32,\\\"CreateEmptyFile\\\":false,\\\"EnableRapidAppends\\\":true,\\\"EnableStreamingWrites\\\":true,\\\"FinalizeFileForRapid\\\":false,\\\"GlobalMaxBlocks\\\":4,\\\"MaxBlocksPerFile\\\":1}}}\\n\",\n",
    "      \"{\\\"timestamp\\\":{\\\"seconds\\\":1765594432,\\\"nanos\\\":650405114},\\\"severity\\\":\\\"INFO\\\",\\\"message\\\":\\\"File system has been successfully mounted.\\\",\\\"mount-id\\\":\\\"pegasus_cnn_daily_mail_2k-55dafd95\\\"}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"!pip install -U transformers\\n\",\n",
    "    \"!pip install -U datasets\\n\",\n",
    "    \"!pip install tensorboard\\n\",\n",
    "    \"!pip install sentencepiece\\n\",\n",
    "    \"!pip install accelerate\\n\",\n",
    "    \"!pip install evaluate\\n\",\n",
    "    \"!pip install rouge_score\\n\",\n",
    "    \"!pip install tqdm\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"mWoALIEfzkzm\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\"\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765592972299,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 40191,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    },\n",
    "    \"outputId\": \"a14b6e0e-1d38-44e7-d995-b9d5365698a8\"\n",
    "   },\n",
    "   \"execution_count\": 3,\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stdout\",\n",
    "     \"text\": [\n",
    "      \"Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\\n\",\n",
    "      \"Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\\n\",\n",
    "      \"Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\\n\",\n",
    "      \"Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\\n\",\n",
    "      \"Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\\n\",\n",
    "      \"Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\\n\",\n",
    "      \"Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\\n\",\n",
    "      \"Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\\n\",\n",
    "      \"Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\\n\",\n",
    "      \"Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\\n\",\n",
    "      \"Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\\n\",\n",
    "      \"Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\\n\",\n",
    "      \"Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\\n\",\n",
    "      \"Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\\n\",\n",
    "      \"Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\\n\",\n",
    "      \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\\n\",\n",
    "      \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\\n\",\n",
    "      \"Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\\n\",\n",
    "      \"Collecting datasets\\n\",\n",
    "      \"  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\\n\",\n",
    "      \"Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\\n\",\n",
    "      \"Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\\n\",\n",
    "      \"Collecting pyarrow>=21.0.0 (from datasets)\\n\",\n",
    "      \"  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\\n\",\n",
    "      \"Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\\n\",\n",
    "      \"Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\\n\",\n",
    "      \"Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\\n\",\n",
    "      \"Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\\n\",\n",
    "      \"Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\\n\",\n",
    "      \"Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\\n\",\n",
    "      \"Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\\n\",\n",
    "      \"Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\\n\",\n",
    "      \"Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\\n\",\n",
    "      \"Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\\n\",\n",
    "      \"Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\\n\",\n",
    "      \"Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.0)\\n\",\n",
    "      \"Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\\n\",\n",
    "      \"Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\\n\",\n",
    "      \"Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\\n\",\n",
    "      \"Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\\n\",\n",
    "      \"Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\\n\",\n",
    "      \"Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\\n\",\n",
    "      \"Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\\n\",\n",
    "      \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\\n\",\n",
    "      \"Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\\n\",\n",
    "      \"Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\\n\",\n",
    "      \"Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\\n\",\n",
    "      \"Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\\n\",\n",
    "      \"Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\\n\",\n",
    "      \"Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\\n\",\n",
    "      \"Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\\n\",\n",
    "      \"Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\\n\",\n",
    "      \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\\n\",\n",
    "      \"Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\\n\",\n",
    "      \"\\u001B[2K   \\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\u001B[0m \\u001B[32m511.6/511.6 kB\\u001B[0m \\u001B[31m36.5 MB/s\\u001B[0m eta \\u001B[36m0:00:00\\u001B[0m\\n\",\n",
    "      \"\\u001B[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\\n\",\n",
    "      \"\\u001B[2K   \\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\u001B[0m \\u001B[32m47.7/47.7 MB\\u001B[0m \\u001B[31m51.7 MB/s\\u001B[0m eta \\u001B[36m0:00:00\\u001B[0m\\n\",\n",
    "      \"\\u001B[?25hInstalling collected packages: pyarrow, datasets\\n\",\n",
    "      \"  Attempting uninstall: pyarrow\\n\",\n",
    "      \"    Found existing installation: pyarrow 18.1.0\\n\",\n",
    "      \"    Uninstalling pyarrow-18.1.0:\\n\",\n",
    "      \"      Successfully uninstalled pyarrow-18.1.0\\n\",\n",
    "      \"  Attempting uninstall: datasets\\n\",\n",
    "      \"    Found existing installation: datasets 4.0.0\\n\",\n",
    "      \"    Uninstalling datasets-4.0.0:\\n\",\n",
    "      \"      Successfully uninstalled datasets-4.0.0\\n\",\n",
    "      \"Successfully installed datasets-4.4.1 pyarrow-22.0.0\\n\",\n",
    "      \"Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\\n\",\n",
    "      \"Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\\n\",\n",
    "      \"Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\\n\",\n",
    "      \"Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\\n\",\n",
    "      \"Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\\n\",\n",
    "      \"Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\\n\",\n",
    "      \"Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\\n\",\n",
    "      \"Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\\n\",\n",
    "      \"Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\\n\",\n",
    "      \"Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.4)\\n\",\n",
    "      \"Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\\n\",\n",
    "      \"Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\\n\",\n",
    "      \"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\\n\",\n",
    "      \"Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\\n\",\n",
    "      \"Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\\n\",\n",
    "      \"Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\\n\",\n",
    "      \"Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\\n\",\n",
    "      \"Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\\n\",\n",
    "      \"Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\\n\",\n",
    "      \"Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\\n\",\n",
    "      \"Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\\n\",\n",
    "      \"Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\\n\",\n",
    "      \"Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\\n\",\n",
    "      \"Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\\n\",\n",
    "      \"Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\\n\",\n",
    "      \"Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\\n\",\n",
    "      \"Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\\n\",\n",
    "      \"Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\\n\",\n",
    "      \"Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\\n\",\n",
    "      \"Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\\n\",\n",
    "      \"Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\\n\",\n",
    "      \"Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\\n\",\n",
    "      \"Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\\n\",\n",
    "      \"Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\\n\",\n",
    "      \"Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\\n\",\n",
    "      \"Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\\n\",\n",
    "      \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\\n\",\n",
    "      \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\\n\",\n",
    "      \"Collecting evaluate\\n\",\n",
    "      \"  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\\n\",\n",
    "      \"Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.1)\\n\",\n",
    "      \"Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\\n\",\n",
    "      \"Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\\n\",\n",
    "      \"Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\\n\",\n",
    "      \"Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\\n\",\n",
    "      \"Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\\n\",\n",
    "      \"Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\\n\",\n",
    "      \"Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\\n\",\n",
    "      \"Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\\n\",\n",
    "      \"Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\\n\",\n",
    "      \"Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\\n\",\n",
    "      \"Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\\n\",\n",
    "      \"Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\\n\",\n",
    "      \"Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\\n\",\n",
    "      \"Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\\n\",\n",
    "      \"Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\\n\",\n",
    "      \"Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\\n\",\n",
    "      \"Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\\n\",\n",
    "      \"Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\\n\",\n",
    "      \"Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\\n\",\n",
    "      \"Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\\n\",\n",
    "      \"Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\\n\",\n",
    "      \"Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\\n\",\n",
    "      \"Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\\n\",\n",
    "      \"Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\\n\",\n",
    "      \"Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\\n\",\n",
    "      \"Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\\n\",\n",
    "      \"Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\\n\",\n",
    "      \"Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\\n\",\n",
    "      \"Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\\n\",\n",
    "      \"Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\\n\",\n",
    "      \"Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.0)\\n\",\n",
    "      \"Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\\n\",\n",
    "      \"Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\\n\",\n",
    "      \"Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\\n\",\n",
    "      \"Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\\n\",\n",
    "      \"\\u001B[2K   \\u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\\u001B[0m \\u001B[32m84.1/84.1 kB\\u001B[0m \\u001B[31m7.2 MB/s\\u001B[0m eta \\u001B[36m0:00:00\\u001B[0m\\n\",\n",
    "      \"\\u001B[?25hInstalling collected packages: evaluate\\n\",\n",
    "      \"Successfully installed evaluate-0.4.6\\n\",\n",
    "      \"Collecting rouge_score\\n\",\n",
    "      \"  Downloading rouge_score-0.1.2.tar.gz (17 kB)\\n\",\n",
    "      \"  Preparing metadata (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n",
    "      \"Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\\n\",\n",
    "      \"Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\\n\",\n",
    "      \"Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\\n\",\n",
    "      \"Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\\n\",\n",
    "      \"Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\\n\",\n",
    "      \"Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\\n\",\n",
    "      \"Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\\n\",\n",
    "      \"Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\\n\",\n",
    "      \"Building wheels for collected packages: rouge_score\\n\",\n",
    "      \"  Building wheel for rouge_score (setup.py) ... \\u001B[?25l\\u001B[?25hdone\\n\",\n",
    "      \"  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c251300c9e6367767d08d9e1064775fe6798dad86da97f0c59fd6eb38584c619\\n\",\n",
    "      \"  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\\n\",\n",
    "      \"Successfully built rouge_score\\n\",\n",
    "      \"Installing collected packages: rouge_score\\n\",\n",
    "      \"Successfully installed rouge_score-0.1.2\\n\",\n",
    "      \"Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import pprint\\n\",\n",
    "    \"import evaluate\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from torch.utils.tensorboard import SummaryWriter\\n\",\n",
    "    \"\\n\",\n",
    "    \"from transformers import (\\n\",\n",
    "    \"    PegasusForConditionalGeneration,  Trainer, TrainingArguments,\\n\",\n",
    "    \"    PegasusTokenizer,EarlyStoppingCallback,T5ForConditionalGeneration, T5Tokenizer,\\n\",\n",
    "    \"    PegasusXForConditionalGeneration,\\n\",\n",
    "    \"    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"from datasets import load_dataset\\n\",\n",
    "    \"\\n\",\n",
    "    \"pp = pprint.PrettyPrinter()\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"89nzRnGjz3CL\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594455382,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 12355,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    }\n",
    "   },\n",
    "   \"execution_count\": 5,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\\n\",\n",
    "    \"  \\\"\\\"\\\"\\n\",\n",
    "    \"  Prepare configurations and base model for fine-tuning\\n\",\n",
    "    \"  \\\"\\\"\\\"\\n\",\n",
    "    \"  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\\n\",\n",
    "    \"  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  if freeze_encoder:\\n\",\n",
    "    \"    for param in model.model.encoder.parameters():\\n\",\n",
    "    \"      param.requires_grad = False\\n\",\n",
    "    \"\\n\",\n",
    "    \"  if val_dataset is not None:\\n\",\n",
    "    \"    training_args = Seq2SeqTrainingArguments(\\n\",\n",
    "    \"      output_dir=OUT_DIR,           # output directory\\n\",\n",
    "    \"      num_train_epochs=10,           # total number of training epochs\\n\",\n",
    "    \"      per_device_train_batch_size=8,   # batch size per device during training, can increase if memory allows\\n\",\n",
    "    \"      per_device_eval_batch_size=16,    # batch size for evaluation, can increase if memory allows\\n\",\n",
    "    \"      save_steps=100,                  # number of updates steps before checkpoint saves\\n\",\n",
    "    \"      save_total_limit=10,              # limit the total amount of checkpoints and deletes the older checkpoints\\n\",\n",
    "    \"      # eval_strategy='steps',     # evaluation strategy to adopt during training\\n\",\n",
    "    \"      warmup_steps=100,                # number of warmup steps for learning rate scheduler\\n\",\n",
    "    \"      weight_decay=0.01,               # strength of weight decay\\n\",\n",
    "    \"      logging_dir=f'{OUT_DIR}/logs'\\n\",\n",
    "    \"      ,            # directory for storing logs\\n\",\n",
    "    \"      learning_rate=0.0001,\\n\",\n",
    "    \"      predict_with_generate=True,\\n\",\n",
    "    \"      bf16=True,\\n\",\n",
    "    \"      fp16=False,\\n\",\n",
    "    \"      tf32=True,\\n\",\n",
    "    \"\\n\",\n",
    "    \"      logging_steps=200,\\n\",\n",
    "    \"      gradient_accumulation_steps=8,\\n\",\n",
    "    \"      logging_strategy=\\\"epoch\\\",\\n\",\n",
    "    \"    # eval_strategy='steps',\\n\",\n",
    "    \"      eval_strategy='epoch',\\n\",\n",
    "    \"      eval_steps=200,\\n\",\n",
    "    \"      save_strategy='epoch',\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"      torch_compile=True,\\n\",\n",
    "    \"      report_to='tensorboard',\\n\",\n",
    "    \"      metric_for_best_model='eval_loss',\\n\",\n",
    "    \"      greater_is_better=False,\\n\",\n",
    "    \"      load_best_model_at_end=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    trainer = Seq2SeqTrainer(\\n\",\n",
    "    \"      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\\n\",\n",
    "    \"      args=training_args,                  # training arguments, defined above\\n\",\n",
    "    \"      train_dataset=train_dataset,         # training dataset\\n\",\n",
    "    \"      eval_dataset=val_dataset,            # evaluation dataset\\n\",\n",
    "    \"      tokenizer=tokenizer,\\n\",\n",
    "    \"      compute_metrics=compute_metrics,\\n\",\n",
    "    \"      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"  else:\\n\",\n",
    "    \"    training_args = Seq2SeqTrainingArguments(\\n\",\n",
    "    \"      output_dir=OUT_DIR,           # output directory\\n\",\n",
    "    \"      num_train_epochs=10,           # total number of training epochs\\n\",\n",
    "    \"      per_device_train_batch_size=8,   # batch size per device during training, can increase if memory allows\\n\",\n",
    "    \"      save_steps=100,                  # number of updates steps before checkpoint saves\\n\",\n",
    "    \"      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\\n\",\n",
    "    \"      warmup_steps=100,                # number of warmup steps for learning rate scheduler\\n\",\n",
    "    \"      weight_decay=0.01,               # strength of weight decay\\n\",\n",
    "    \"      logging_dir='./logs',            # directory for storing logs\\n\",\n",
    "    \"      logging_steps=100,\\n\",\n",
    "    \"      learning_rate=0.0001,\\n\",\n",
    "    \"      predict_with_generate=True,\\n\",\n",
    "    \"      report_to='tensorboard',\\n\",\n",
    "    \"      eval_strategy='steps',     # evaluation strategy to adopt during training\\n\",\n",
    "    \"      eval_steps=100,                  # number of update steps before evaluation\\n\",\n",
    "    \"    bf16=True,\\n\",\n",
    "    \"    fp16=False,\\n\",\n",
    "    \"    tf32=True,\\n\",\n",
    "    \"\\n\",\n",
    "    \"    torch_compile=True,\\n\",\n",
    "    \"          metric_for_best_model='eval_loss',\\n\",\n",
    "    \"    greater_is_better=False,\\n\",\n",
    "    \"    load_best_model_at_end=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    trainer = Seq2SeqTrainer(\\n\",\n",
    "    \"      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\\n\",\n",
    "    \"      args=training_args,                  # training arguments, defined above\\n\",\n",
    "    \"      train_dataset=train_dataset,         # training dataset\\n\",\n",
    "    \"      tokenizer=tokenizer,\\n\",\n",
    "    \"      compute_metrics=compute_metrics,\\n\",\n",
    "    \"      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"  return trainer\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"4NGDTnQ1wuHZ\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594455396,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 13,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    }\n",
    "   },\n",
    "   \"execution_count\": 6,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from torch.utils.tensorboard import SummaryWriter\\n\",\n",
    "    \"from transformers import TrainerCallback\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"writer = SummaryWriter(log_dir=OUT_DIR)\\n\",\n",
    "    \"\\n\",\n",
    "    \"class PegasusDataset(torch.utils.data.Dataset):\\n\",\n",
    "    \"    def __init__(self, encodings, labels):\\n\",\n",
    "    \"        self.encodings = encodings\\n\",\n",
    "    \"        self.labels = labels\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __getitem__(self, idx):\\n\",\n",
    "    \"        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\\n\",\n",
    "    \"        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\\n\",\n",
    "    \"        return item\\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        return len(self.labels['input_ids'])  # len(self.labels)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def on_step_end(self, args, state, control, **kwargs):\\n\",\n",
    "    \"        if torch.cuda.is_available():\\n\",\n",
    "    \"            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\\n\",\n",
    "    \"        return control\\n\",\n",
    "    \"\\n\",\n",
    "    \"class GpuLoggerCallback(TrainerCallback):\\n\",\n",
    "    \"    def __init__(self, writer):\\n\",\n",
    "    \"        self.writer = writer\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def on_step_end(self, args, state, control, **kwargs):\\n\",\n",
    "    \"        if torch.cuda.is_available():\\n\",\n",
    "    \"            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\\n\",\n",
    "    \"            self.writer.add_scalar(\\\"gpu_memory_gb\\\", gpu_mem, state.global_step)\\n\",\n",
    "    \"        return control\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"def prepare_data(model_name,\\n\",\n",
    "    \"                 train_texts, train_labels,\\n\",\n",
    "    \"                 val_texts=None, val_labels=None,\\n\",\n",
    "    \"                 test_texts=None, test_labels=None):\\n\",\n",
    "    \"  \\\"\\\"\\\"\\n\",\n",
    "    \"  Prepare input data for model fine-tuning\\n\",\n",
    "    \"  \\\"\\\"\\\"\\n\",\n",
    "    \"  tokenizer = PegasusTokenizer.from_pretrained(model_name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  prepare_val = False if val_texts is None or val_labels is None else True\\n\",\n",
    "    \"  prepare_test = False if test_texts is None or test_labels is None else True\\n\",\n",
    "    \"\\n\",\n",
    "    \"  def tokenize_data(texts, labels):\\n\",\n",
    "    \"    encodings = tokenizer(texts, truncation=True, padding=True)\\n\",\n",
    "    \"    decodings = tokenizer(labels, truncation=True, padding=True)\\n\",\n",
    "    \"    dataset_tokenized = PegasusDataset(encodings, decodings)\\n\",\n",
    "    \"    return dataset_tokenized\\n\",\n",
    "    \"\\n\",\n",
    "    \"  train_dataset = tokenize_data(train_texts, train_labels)\\n\",\n",
    "    \"  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\\n\",\n",
    "    \"  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\\n\",\n",
    "    \"\\n\",\n",
    "    \"  return train_dataset, val_dataset, test_dataset, tokenizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"def compute_metrics(eval_pred):\\n\",\n",
    "    \"    rouge = evaluate.load(\\\"rouge\\\")\\n\",\n",
    "    \"    # predictions, labels = eval_pred.predictions[0], eval_pred.label_ids[0]\\n\",\n",
    "    \"    predictions, labels = eval_pred.predictions, eval_pred.label_ids\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Decode the predictions\\n\",\n",
    "    \"    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Replace -100 in labels before decoding\\n\",\n",
    "    \"    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n\",\n",
    "    \"    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Compute ROUGE\\n\",\n",
    "    \"    rouge_result = rouge.compute(\\n\",\n",
    "    \"        predictions=decoded_preds,\\n\",\n",
    "    \"        references=decoded_labels,\\n\",\n",
    "    \"        use_stemmer=True,\\n\",\n",
    "    \"        rouge_types=[\\\"rouge1\\\", \\\"rouge2\\\", \\\"rougeL\\\", \\\"rougeLsum\\\"]\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\\n\",\n",
    "    \"    rouge_result[\\\"gen_len\\\"] = np.mean(prediction_lens)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # NEW: Log to TensorBoard\\n\",\n",
    "    \"    for k, v in rouge_result.items():\\n\",\n",
    "    \"        writer.add_scalar(f\\\"eval/{k}\\\", v, trainer.state.global_step)\\n\",\n",
    "    \"    # Print rounded values\\n\",\n",
    "    \"    # pprint.print({k: round(v, 4) for k, v in rouge_result.items()})\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # Must return a *dict*, not a set\\n\",\n",
    "    \"    return {k: v for k, v in rouge_result.items()}\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"--QyWKjbweCG\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594458373,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 2976,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    }\n",
    "   },\n",
    "   \"execution_count\": 7,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from datasets import Dataset\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"  # train_texts, train_labels = dataset['train']['document'][:1000], dataset['train']['summary'][:1000]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Loading data...\\\")\\n\",\n",
    "    \"train_df = pd.read_csv(f\\\"{DRIVE_DATA_PATH}/train.csv\\\").head(2000)\\n\",\n",
    "    \"val_df = pd.read_csv(f\\\"{DRIVE_DATA_PATH}/val.csv\\\").head(400)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"dataset_train = Dataset.from_pandas(train_df)\\n\",\n",
    "    \"dataset_valid = Dataset.from_pandas(val_df)\\n\",\n",
    "    \"train_texts, train_labels = dataset_train[CLEAN_TEXT_COLUMN][:2000], dataset_train[SUMMARY_COLUMN][:2000]\\n\",\n",
    "    \"valid_texts, valid_labels = dataset_valid[CLEAN_TEXT_COLUMN][:2000], dataset_valid[SUMMARY_COLUMN][:2000]\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# train_texts = train_df[CLEAN_TEXT_COLUMN]\\n\",\n",
    "    \"# train_labels = train_df[SUMMARY_COLUMN]\\n\",\n",
    "    \"print(\\\"Train:\\\", len(train_texts))\\n\",\n",
    "    \"print(\\\"Val:\\\", len(valid_texts))\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_dataset, val_dataset, test_dataset, tokenizer = prepare_data(MODEL, train_texts, train_labels, valid_texts, valid_labels)\\n\",\n",
    "    \"trainer = prepare_fine_tuning(MODEL, tokenizer, train_dataset, val_dataset=val_dataset)\\n\",\n",
    "    \"trainer.train()\\n\",\n",
    "    \"trainer.add_callback(GpuLoggerCallback(writer))\\n\",\n",
    "    \"# start = time.time()\\n\",\n",
    "    \"# history = trainer.train()\\n\",\n",
    "    \"# end = time.time()\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# dataset_train = Dataset.from_pandas(train_df)\\n\",\n",
    "    \"# dataset_valid = Dataset.from_pandas(val_df)\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"SvN3sG6Ixhio\",\n",
    "    \"colab\": {\n",
    "     \"base_uri\": \"https://localhost:8080/\",\n",
    "     \"height\": 659\n",
    "    },\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765604652939,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 10190417,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    },\n",
    "    \"outputId\": \"8ee521df-8b95-4fe4-d9cd-e4fde4e8d244\"\n",
    "   },\n",
    "   \"execution_count\": 8,\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stdout\",\n",
    "     \"text\": [\n",
    "      \"Loading data...\\n\",\n",
    "      \"Train: 2000\\n\",\n",
    "      \"Val: 400\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stderr\",\n",
    "     \"text\": [\n",
    "      \"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\\n\",\n",
    "      \"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\\n\",\n",
    "      \"/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\\n\",\n",
    "      \"  self.setter(val)\\n\",\n",
    "      \"/tmp/ipython-input-1196698668.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\\n\",\n",
    "      \"  trainer = Seq2SeqTrainer(\\n\",\n",
    "      \"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\\n\",\n",
    "      \"W1213 02:54:50.843000 12081 torch/fx/experimental/symbolic_shapes.py:6833] [8/1] _maybe_guard_rel() was called on non-relation expression Eq(s50, s7) | Eq(s7, 1)\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"output_type\": \"display_data\",\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ],\n",
    "      \"text/html\": [\n",
    "       \"\\n\",\n",
    "       \"    <div>\\n\",\n",
    "       \"      \\n\",\n",
    "       \"      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\\n\",\n",
    "       \"      [320/320 2:49:19, Epoch 10/10]\\n\",\n",
    "       \"    </div>\\n\",\n",
    "       \"    <table border=\\\"1\\\" class=\\\"dataframe\\\">\\n\",\n",
    "       \"  <thead>\\n\",\n",
    "       \" <tr style=\\\"text-align: left;\\\">\\n\",\n",
    "       \"      <th>Epoch</th>\\n\",\n",
    "       \"      <th>Training Loss</th>\\n\",\n",
    "       \"      <th>Validation Loss</th>\\n\",\n",
    "       \"      <th>Rouge1</th>\\n\",\n",
    "       \"      <th>Rouge2</th>\\n\",\n",
    "       \"      <th>Rougel</th>\\n\",\n",
    "       \"      <th>Rougelsum</th>\\n\",\n",
    "       \"      <th>Gen Len</th>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </thead>\\n\",\n",
    "       \"  <tbody>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>1</td>\\n\",\n",
    "       \"      <td>5.991300</td>\\n\",\n",
    "       \"      <td>4.447385</td>\\n\",\n",
    "       \"      <td>0.471396</td>\\n\",\n",
    "       \"      <td>0.281558</td>\\n\",\n",
    "       \"      <td>0.387959</td>\\n\",\n",
    "       \"      <td>0.388022</td>\\n\",\n",
    "       \"      <td>80.400000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>2</td>\\n\",\n",
    "       \"      <td>5.193100</td>\\n\",\n",
    "       \"      <td>3.888681</td>\\n\",\n",
    "       \"      <td>0.479577</td>\\n\",\n",
    "       \"      <td>0.290775</td>\\n\",\n",
    "       \"      <td>0.389657</td>\\n\",\n",
    "       \"      <td>0.388857</td>\\n\",\n",
    "       \"      <td>90.520000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>3</td>\\n\",\n",
    "       \"      <td>4.698000</td>\\n\",\n",
    "       \"      <td>3.447221</td>\\n\",\n",
    "       \"      <td>0.483030</td>\\n\",\n",
    "       \"      <td>0.295052</td>\\n\",\n",
    "       \"      <td>0.396231</td>\\n\",\n",
    "       \"      <td>0.395479</td>\\n\",\n",
    "       \"      <td>84.480000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>4</td>\\n\",\n",
    "       \"      <td>3.911500</td>\\n\",\n",
    "       \"      <td>2.090791</td>\\n\",\n",
    "       \"      <td>0.492290</td>\\n\",\n",
    "       \"      <td>0.301355</td>\\n\",\n",
    "       \"      <td>0.403541</td>\\n\",\n",
    "       \"      <td>0.403482</td>\\n\",\n",
    "       \"      <td>85.760000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>5</td>\\n\",\n",
    "       \"      <td>2.063700</td>\\n\",\n",
    "       \"      <td>0.611953</td>\\n\",\n",
    "       \"      <td>0.485830</td>\\n\",\n",
    "       \"      <td>0.292656</td>\\n\",\n",
    "       \"      <td>0.393225</td>\\n\",\n",
    "       \"      <td>0.392335</td>\\n\",\n",
    "       \"      <td>81.160000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>6</td>\\n\",\n",
    "       \"      <td>0.771400</td>\\n\",\n",
    "       \"      <td>0.522189</td>\\n\",\n",
    "       \"      <td>0.484842</td>\\n\",\n",
    "       \"      <td>0.289569</td>\\n\",\n",
    "       \"      <td>0.392326</td>\\n\",\n",
    "       \"      <td>0.391543</td>\\n\",\n",
    "       \"      <td>81.760000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>7</td>\\n\",\n",
    "       \"      <td>0.562700</td>\\n\",\n",
    "       \"      <td>0.496487</td>\\n\",\n",
    "       \"      <td>0.485515</td>\\n\",\n",
    "       \"      <td>0.293358</td>\\n\",\n",
    "       \"      <td>0.396059</td>\\n\",\n",
    "       \"      <td>0.395768</td>\\n\",\n",
    "       \"      <td>80.440000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>8</td>\\n\",\n",
    "       \"      <td>0.493200</td>\\n\",\n",
    "       \"      <td>0.483528</td>\\n\",\n",
    "       \"      <td>0.493752</td>\\n\",\n",
    "       \"      <td>0.302902</td>\\n\",\n",
    "       \"      <td>0.403761</td>\\n\",\n",
    "       \"      <td>0.403071</td>\\n\",\n",
    "       \"      <td>81.560000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>9</td>\\n\",\n",
    "       \"      <td>0.460600</td>\\n\",\n",
    "       \"      <td>0.482013</td>\\n\",\n",
    "       \"      <td>0.495155</td>\\n\",\n",
    "       \"      <td>0.303716</td>\\n\",\n",
    "       \"      <td>0.403570</td>\\n\",\n",
    "       \"      <td>0.403337</td>\\n\",\n",
    "       \"      <td>84.560000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"    <tr>\\n\",\n",
    "       \"      <td>10</td>\\n\",\n",
    "       \"      <td>0.447200</td>\\n\",\n",
    "       \"      <td>0.480050</td>\\n\",\n",
    "       \"      <td>0.497840</td>\\n\",\n",
    "       \"      <td>0.307439</td>\\n\",\n",
    "       \"      <td>0.404501</td>\\n\",\n",
    "       \"      <td>0.403890</td>\\n\",\n",
    "       \"      <td>86.160000</td>\\n\",\n",
    "       \"    </tr>\\n\",\n",
    "       \"  </tbody>\\n\",\n",
    "       \"</table><p>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {}\n",
    "    },\n",
    "    {\n",
    "     \"output_type\": \"stream\",\n",
    "     \"name\": \"stderr\",\n",
    "     \"text\": [\n",
    "      \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\\n\",\n",
    "      \"  warnings.warn(\\n\",\n",
    "      \"There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"# def compute_metrics(eval_pred):\\n\",\n",
    "    \"#     # predictions, labels = eval_pred\\n\",\n",
    "    \"#     predictions, labels = eval_pred.predictions, eval_pred.label_ids\\n\",\n",
    "    \"#     pprint(predictions)\\n\",\n",
    "    \"#     pprint(labels)\\n\",\n",
    "    \"#     # eval_pred.predictions, eval_pred.label_ids\\n\",\n",
    "    \"#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#     # Replace -100\\n\",\n",
    "    \"#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\\n\",\n",
    "    \"#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#     # ROUGE\\n\",\n",
    "    \"#     rouge_result = rouge.compute(\\n\",\n",
    "    \"#         predictions=decoded_preds,\\n\",\n",
    "    \"#         references=decoded_labels,\\n\",\n",
    "    \"#         use_stemmer=True,\\n\",\n",
    "    \"#         rouge_types=[\\\"rouge1\\\", \\\"rouge2\\\", \\\"rougeL\\\", \\\"rougeLsum\\\"]\\n\",\n",
    "    \"#     )\\n\",\n",
    "    \"\\n\",\n",
    "    \"#     pprint({k: round(v, 4) for k, v in rouge_result.items()})\\n\",\n",
    "    \"\\n\",\n",
    "    \"#     return {\\n\",\n",
    "    \"#         {k: v for k, v in rouge_result.items()}\\n\",\n",
    "    \"#     }\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"aJQo25WJ9n3i\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from transformers.trainer_utils import get_last_checkpoint\\n\",\n",
    "    \"path = f'results'\\n\",\n",
    "    \"last_ckpt = get_last_checkpoint(path)\\n\",\n",
    "    \"model_path = last_ckpt if last_ckpt else OUT_DIR\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = PegasusForConditionalGeneration.from_pretrained(model_path)\\n\",\n",
    "    \"tokenizer = PegasusForConditionalGeneration.from_pretrained(model_path)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# model = T5ForConditionalGeneration.from_pretrained\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"xPYmhfaEEXnu\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\\n\",\n",
    "    \"    # 1. Tokenize properly (returns attention mask too)\\n\",\n",
    "    \"    encoded = tokenizer(\\n\",\n",
    "    \"        \\\"summarize: \\\" + text,\\n\",\n",
    "    \"        return_tensors='pt',\\n\",\n",
    "    \"        max_length=max_length,\\n\",\n",
    "    \"        truncation=True,\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 2. Move everything to the same device as the model\\n\",\n",
    "    \"    device = model.device\\n\",\n",
    "    \"    encoded = {k: v.to(device) for k, v in encoded.items()}\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 3. Generate summary\\n\",\n",
    "    \"    summary_ids = model.generate(\\n\",\n",
    "    \"        **encoded,\\n\",\n",
    "    \"        max_length=128,         # not 50 â†’ 50 is too short for news\\n\",\n",
    "    \"        num_beams=num_beams,\\n\",\n",
    "    \"        length_penalty=1.1,\\n\",\n",
    "    \"        no_repeat_ngram_size=3,\\n\",\n",
    "    \"        early_stopping=True\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 4. Decode\\n\",\n",
    "    \"    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TOSBh8AwElEk\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"nltk.download(\\\"punkt\\\")\\n\",\n",
    "    \"nltk.download(\\\"punkt_tab\\\")\\n\",\n",
    "    \"device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n\",\n",
    "    \"nli_tok = AutoTokenizer.from_pretrained(\\\"roberta-large-mnli\\\")\\n\",\n",
    "    \"nli_model = AutoModelForSequenceClassification.from_pretrained(\\\"roberta-large-mnli\\\").to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"def hallucination_rate(summary, source):\\n\",\n",
    "    \"    sentences = nltk.sent_tokenize(summary)\\n\",\n",
    "    \"    hallucinated = 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for sent in sentences:\\n\",\n",
    "    \"        inputs = nli_tok.encode_plus(source, sent, return_tensors=\\\"pt\\\", truncation=True).to(device)\\n\",\n",
    "    \"        logits = nli_model(**inputs).logits\\n\",\n",
    "    \"        probs = torch.softmax(logits, dim=1)[0]\\n\",\n",
    "    \"        contradiction = probs[0].item()\\n\",\n",
    "    \"        entailment = probs[2].item()\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if contradiction > entailment:\\n\",\n",
    "    \"            hallucinated += 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"    return hallucinated / len(sentences)\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TdVCvy2iEoLP\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"test_df = pd.read_csv(f\\\"{DRIVE_DATA_PATH}/test.csv\\\")\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"3hx8hlz_Es-e\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"!rm -rf '/content/results-pegasus'\\n\",\n",
    "    \"# !rm -rf '/content/results_t5_large_regularized'\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"uQqdyxHkWfRO\",\n",
    "    \"executionInfo\": {\n",
    "     \"status\": \"ok\",\n",
    "     \"timestamp\": 1765594379811,\n",
    "     \"user_tz\": -60,\n",
    "     \"elapsed\": 110,\n",
    "     \"user\": {\n",
    "      \"displayName\": \"Laurent Arifaj\",\n",
    "      \"userId\": \"09689383856083908659\"\n",
    "     }\n",
    "    }\n",
    "   },\n",
    "   \"execution_count\": 3,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"from google.colab import auth\\n\",\n",
    "    \"auth.authenticate_user()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Install gcsfuse\\n\",\n",
    "    \"!echo \\\"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\\\" > /etc/apt/sources.list.d/gcsfuse.list\\n\",\n",
    "    \"!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\\n\",\n",
    "    \"!apt -qq update\\n\",\n",
    "    \"!apt -qq install gcsfuse\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create a local directory for mounting\\n\",\n",
    "    \"# !mkdir results_t5base\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Mount the GCS bucket\\n\",\n",
    "    \"# Replace 'your-bucket-name' with the actual name of your GCS bucket\\n\",\n",
    "    \"# !gcsfuse --implicit-dirs models_checkpoint results_t5base\\n\",\n",
    "    \"\\n\",\n",
    "    \"!gsutil -m cp -r /content/results-pegasus/ gs://pegasus_2k_model/models/results_pegasus-large/results\\n\",\n",
    "    \"\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"UVDaIZOWWzMB\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"source\": [\n",
    "    \"!gsutil -m cp -r /content/logs/ gs://pegasus_2k_model/models/results_pegasus-large/logs\\n\"\n",
    "   ],\n",
    "   \"metadata\": {\n",
    "    \"id\": \"CZ6gp72cr-PG\"\n",
    "   },\n",
    "   \"execution_count\": null,\n",
    "   \"outputs\": []\n",
    "  }\n",
    " ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25a81074a641a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
