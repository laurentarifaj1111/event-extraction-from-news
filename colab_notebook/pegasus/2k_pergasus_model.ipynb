{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "authorship_tag": "ABX9TyNSuFzF092bQkiHLxKHY6sZ"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "EMIflcpRwSqn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594364498,
     "user_tz": -60,
     "elapsed": 7,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    }
   },
   "source": [
    "MODEL = 'google/pegasus-cnn_dailymail'\n",
    "# MODEL = 'google/pegasus-cnn_dailymail'\n",
    "\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = 4\n",
    "EPOCHS = 10\n",
    "OUT_DIR = 'results-pegasus/2k_samples'\n",
    "MAX_LENGTH = 1024 # Maximum context length to consider while preparing dataset.\n",
    "epoch_metrics = []\n",
    "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/10k_samples\"   # UPDATE PATH\n",
    "CLEAN_TEXT_COLUMN='article'\n",
    "SUMMARY_COLUMN='highlights'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "id": "DubrCb8GwXyg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594367574,
     "user_tz": -60,
     "elapsed": 1465,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    },
    "outputId": "5ff9e8ff-3bf6-4db2-9f62-99250c32bd41"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Install gcsfuse\n",
    "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
    "!apt -qq update\n",
    "!apt -qq install gcsfuse\n",
    "\n",
    "\n",
    "# Create a local directory for mounting\n",
    "!mkdir results-pegasus\n",
    "# Mount the GCS bucket\n",
    "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
    "!gcsfuse --implicit-dirs pegasus_cnn_daily_mail_2k results-pegasus"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHjT8Kkrn3CH",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594432887,
     "user_tz": -60,
     "elapsed": 34614,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    },
    "outputId": "8ab27aeb-1404-48ae-a4ad-390c21fd873c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U transformers\n",
    "!pip install -U datasets\n",
    "!pip install tensorboard\n",
    "!pip install sentencepiece\n",
    "!pip install accelerate\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install tqdm"
   ],
   "metadata": {
    "id": "mWoALIEfzkzm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765592972299,
     "user_tz": -60,
     "elapsed": 40191,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    },
    "outputId": "a14b6e0e-1d38-44e7-d995-b9d5365698a8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pprint\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    PegasusForConditionalGeneration,  Trainer, TrainingArguments,\n",
    "    PegasusTokenizer,EarlyStoppingCallback,T5ForConditionalGeneration, T5Tokenizer,\n",
    "    PegasusXForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n"
   ],
   "metadata": {
    "id": "89nzRnGjz3CL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594455382,
     "user_tz": -60,
     "elapsed": 12355,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def prepare_fine_tuning(model_name, tokenizer, train_dataset, val_dataset=None, freeze_encoder=False, output_dir='./results'):\n",
    "  \"\"\"\n",
    "  Prepare configurations and base model for fine-tuning\n",
    "  \"\"\"\n",
    "  torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "  model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "  if freeze_encoder:\n",
    "    for param in model.model.encoder.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  if val_dataset is not None:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=OUT_DIR,           # output directory\n",
    "      num_train_epochs=10,           # total number of training epochs\n",
    "      per_device_train_batch_size=8,   # batch size per device during training, can increase if memory allows\n",
    "      per_device_eval_batch_size=16,    # batch size for evaluation, can increase if memory allows\n",
    "      save_steps=100,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=10,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      # eval_strategy='steps',     # evaluation strategy to adopt during training\n",
    "      warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir=f'{OUT_DIR}/logs'\n",
    "      ,            # directory for storing logs\n",
    "      learning_rate=0.0001,\n",
    "      predict_with_generate=True,\n",
    "      bf16=True,\n",
    "      fp16=False,\n",
    "      tf32=True,\n",
    "\n",
    "      logging_steps=200,\n",
    "      gradient_accumulation_steps=8,\n",
    "      logging_strategy=\"epoch\",\n",
    "    # eval_strategy='steps',\n",
    "      eval_strategy='epoch',\n",
    "      eval_steps=200,\n",
    "      save_strategy='epoch',\n",
    "\n",
    "\n",
    "      torch_compile=True,\n",
    "      report_to='tensorboard',\n",
    "      metric_for_best_model='eval_loss',\n",
    "      greater_is_better=False,\n",
    "      load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      eval_dataset=val_dataset,            # evaluation dataset\n",
    "      tokenizer=tokenizer,\n",
    "      compute_metrics=compute_metrics,\n",
    "      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "      output_dir=OUT_DIR,           # output directory\n",
    "      num_train_epochs=10,           # total number of training epochs\n",
    "      per_device_train_batch_size=8,   # batch size per device during training, can increase if memory allows\n",
    "      save_steps=100,                  # number of updates steps before checkpoint saves\n",
    "      save_total_limit=5,              # limit the total amount of checkpoints and deletes the older checkpoints\n",
    "      warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "      weight_decay=0.01,               # strength of weight decay\n",
    "      logging_dir='./logs',            # directory for storing logs\n",
    "      logging_steps=100,\n",
    "      learning_rate=0.0001,\n",
    "      predict_with_generate=True,\n",
    "      report_to='tensorboard',\n",
    "      eval_strategy='steps',     # evaluation strategy to adopt during training\n",
    "      eval_steps=100,                  # number of update steps before evaluation\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    tf32=True,\n",
    "\n",
    "    torch_compile=True,\n",
    "          metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "      model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "      args=training_args,                  # training arguments, defined above\n",
    "      train_dataset=train_dataset,         # training dataset\n",
    "      tokenizer=tokenizer,\n",
    "      compute_metrics=compute_metrics,\n",
    "      callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "  return trainer"
   ],
   "metadata": {
    "id": "4NGDTnQ1wuHZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594455396,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import TrainerCallback\n",
    "import time\n",
    "writer = SummaryWriter(log_dir=OUT_DIR)\n",
    "\n",
    "class PegasusDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels['input_ids'])  # len(self.labels)\n",
    "\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        return control\n",
    "\n",
    "class GpuLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, writer):\n",
    "        self.writer = writer\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_mem = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "            self.writer.add_scalar(\"gpu_memory_gb\", gpu_mem, state.global_step)\n",
    "        return control\n",
    "\n",
    "\n",
    "def prepare_data(model_name,\n",
    "                 train_texts, train_labels,\n",
    "                 val_texts=None, val_labels=None,\n",
    "                 test_texts=None, test_labels=None):\n",
    "  \"\"\"\n",
    "  Prepare input data for model fine-tuning\n",
    "  \"\"\"\n",
    "  tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "\n",
    "  prepare_val = False if val_texts is None or val_labels is None else True\n",
    "  prepare_test = False if test_texts is None or test_labels is None else True\n",
    "\n",
    "  def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "    decodings = tokenizer(labels, truncation=True, padding=True)\n",
    "    dataset_tokenized = PegasusDataset(encodings, decodings)\n",
    "    return dataset_tokenized\n",
    "\n",
    "  train_dataset = tokenize_data(train_texts, train_labels)\n",
    "  val_dataset = tokenize_data(val_texts, val_labels) if prepare_val else None\n",
    "  test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
    "\n",
    "  return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    # predictions, labels = eval_pred.predictions[0], eval_pred.label_ids[0]\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "\n",
    "    # Decode the predictions\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels before decoding\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "        rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    )\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    rouge_result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # NEW: Log to TensorBoard\n",
    "    for k, v in rouge_result.items():\n",
    "        writer.add_scalar(f\"eval/{k}\", v, trainer.state.global_step)\n",
    "    # Print rounded values\n",
    "    # pprint.print({k: round(v, 4) for k, v in rouge_result.items()})\n",
    "\n",
    "    # Must return a *dict*, not a set\n",
    "    return {k: v for k, v in rouge_result.items()}"
   ],
   "metadata": {
    "id": "--QyWKjbweCG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765594458373,
     "user_tz": -60,
     "elapsed": 2976,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "  # train_texts, train_labels = dataset['train']['document'][:1000], dataset['train']['summary'][:1000]\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/train.csv\").head(2000)\n",
    "val_df = pd.read_csv(f\"{DRIVE_DATA_PATH}/val.csv\").head(400)\n",
    "\n",
    "\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_valid = Dataset.from_pandas(val_df)\n",
    "train_texts, train_labels = dataset_train[CLEAN_TEXT_COLUMN], dataset_train[SUMMARY_COLUMN]\n",
    "valid_texts, valid_labels = dataset_valid[CLEAN_TEXT_COLUMN], dataset_valid[SUMMARY_COLUMN]\n",
    "\n",
    "\n",
    "print(\"Train:\", len(train_texts))\n",
    "print(\"Val:\", len(valid_texts))\n",
    "\n",
    "\n",
    "train_dataset, val_dataset, test_dataset, tokenizer = prepare_data(MODEL, train_texts, train_labels, valid_texts, valid_labels)\n",
    "trainer = prepare_fine_tuning(MODEL, tokenizer, train_dataset, val_dataset=val_dataset)\n",
    "trainer.train()\n",
    "trainer.add_callback(GpuLoggerCallback(writer))\n"
   ],
   "metadata": {
    "id": "SvN3sG6Ixhio",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765604652939,
     "user_tz": -60,
     "elapsed": 10190417,
     "user": {
      "displayName": "Laurent Arifaj",
      "userId": "09689383856083908659"
     }
    },
    "outputId": "8ee521df-8b95-4fe4-d9cd-e4fde4e8d244"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
