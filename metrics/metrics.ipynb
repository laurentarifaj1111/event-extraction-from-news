{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "MODEL = 't5-base'\n",
        "BATCH_SIZE = 8\n",
        "NUM_PROCS = 4\n",
        "EPOCHS = 10\n",
        "# OUT_DIR = 'results_t5base/2k_samples'\n",
        "OUT_DIR = 'results_t5_large_regularized/10k_samples_fixed'\n",
        "MAX_LENGTH = 1024 # Maximum context length to consider while preparing dataset.\n",
        "epoch_metrics = []\n",
        "DRIVE_DATA_PATH = \"/content/drive/MyDrive/processed/10k_samples\"   # UPDATE PATH\n",
        "CLEAN_TEXT_COLUMN='article'\n",
        "SUMMARY_COLUMN='highlights'"
      ],
      "metadata": {
        "id": "RctCKn0a2Mgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard\n",
        "!pip install tensorboard-data-server\n",
        "!pip install google-cloud-storage\n",
        "!pip install tbparse matplotlib seaborn pandas numpy\n",
        "\n"
      ],
      "metadata": {
        "id": "FHGdehOv2W9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwPmrryU1tgY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !mkdir -p /content/fixed_logs/\n",
        "# !gsutil -m cp -r gs://models_checkpoint/models/results_t5_base_fixed/2k_samples/* /content/fixed_logs/"
      ],
      "metadata": {
        "id": "61rMx4Z64zcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Install gcsfuse\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "\n",
        "# 10k_regularized/10k_samples_fixed/events.out.tfevents.1764724302.669eb4a67fb3.690.2\n",
        "\n",
        "# models_regularized_run/models/results_t5_base/2k_samples/events.out.tfevents.1764788556.fec5b49f987e.2140.3\n",
        "# Create a local directory for mounting\n",
        "!mkdir pegasus_large_50k\n",
        "# models_regularized_run/models/results_t5_base/2k_samples\n",
        "# Mount the GCS bucket\n",
        "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
        "!gcsfuse --implicit-dirs pegasus_large_50k_2nd pegasus_large_50k"
      ],
      "metadata": {
        "id": "ZwWQz0HAURn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a local directory for mounting\n",
        "!mkdir t5_10k_new\n",
        "# models_regularized_run/models/results_t5_base/2k_samples\n",
        "# Mount the GCS bucket\n",
        "# Replace 'your-bucket-name' with the actual name of your GCS bucket\n",
        "!gcsfuse --implicit-dirs t5_large_10_run_test2 t5_10k_new"
      ],
      "metadata": {
        "id": "_lr8L6yAw7Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example run directories\n",
        "# models_regularized_run/models/results_t5_base/2k_samples\n",
        "RUNS = {\n",
        "\"pegasus_large_10k\": '/content/pegasus_large_10k/logs',\n",
        "    \"pegasus_2k\": '/content/pegasus_2k/models/results_pegasus-large/logs',\n",
        "    # \"pegasus_cnn_50k\":'/content/pegasus_cnn_daily_mail_50k/50k_samples',\n",
        "    't5_large_10k':'/content/t5_large/10k_samples_fixed',\n",
        "      't5_large_50k':'/content/t5_large/50k_samples_fixed',\n",
        "    \"10k_regularized\":'/content/10k_regularized/10k_samples_fixed',\n",
        "    't5_large_10_run_10k':'/content/t5_large_10_run/10k_samples_fixed',\n",
        "    't5_large_10_run_50k':'/content/t5_large_10_run/50k_samples_fixed',\n",
        "         't5_10k_new':'/content/t5_10k_new/10k_samples_fixed_modeified_configs'\n",
        "    }\n",
        "\n",
        "\n",
        "RUN_SIZES = {\n",
        "    \"pegasus_large_10k\": 10000,\n",
        "    \"pegasus_2k\": 2000,\n",
        "    \"pegasus_cnn_50k\":50000,\n",
        "    \"t5_large_10k\":10000,\n",
        "    \"t5_large_50k\":50000,\n",
        "    \"10k_regularized\":10000,\n",
        "     \"t5_large_10_run_10k\":10000,\n",
        "    \"t5_large_10_run_50k\":50000,\n",
        "    't5_10k_new':10000\n",
        "}"
      ],
      "metadata": {
        "id": "wEFxinRb8Vff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir '/content/pegasus_large_10k/10k_samples/logs'"
      ],
      "metadata": {
        "id": "Fe6Y0my-5Gwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tbparse import SummaryReader\n",
        "import os\n",
        "\n",
        "def load_run(run_path):\n",
        "    reader = SummaryReader(run_path, pivot=False)\n",
        "    df = reader.scalars  # TensorBoard scalars\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "rYgHkbuo876X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tbparse import SummaryReader\n",
        "\n",
        "files = [f for f in os.listdir('/content/pegasus_cnn_daily_mail_50k/logs') if \"events.out\" in f]\n",
        "\n",
        "for f in files:\n",
        "    fp = os.path.join('/content/pegasus_cnn_daily_mail_50k/logs', f)\n",
        "    print(f\"\\nTrying to load: {fp}\")\n",
        "\n",
        "    try:\n",
        "        reader = SummaryReader(fp, pivot=False)\n",
        "        df = reader.scalars\n",
        "        print(\"Loaded scalars:\", len(df))\n",
        "        print(df.head())\n",
        "    except Exception as e:\n",
        "        print(\"❌ FAILED to load:\", e)\n"
      ],
      "metadata": {
        "id": "prVSIjFnJRgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metrics(run_name, run_path):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    from tbparse import SummaryReader\n",
        "\n",
        "    # Read event logs (auto-detect format: long or wide)\n",
        "    reader = SummaryReader(run_path)\n",
        "    df = reader.scalars\n",
        "\n",
        "    # Detect long vs wide format\n",
        "    is_long_format = \"tag\" in df.columns\n",
        "\n",
        "    # Helper to extract values for a given tag in both formats\n",
        "    def get_values(tag):\n",
        "        if is_long_format:\n",
        "            sub = df[df[\"tag\"] == tag]\n",
        "            return sub[[\"step\", \"value\"]] if not sub.empty else None\n",
        "        else:\n",
        "            if tag in df.columns:\n",
        "                # wide format: 'step' + tag column\n",
        "                sub = df[[\"step\", tag]].dropna()\n",
        "                sub = sub.rename(columns={tag: \"value\"})\n",
        "                return sub if not sub.empty else None\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    # --- 1. TRAIN LOSS ---\n",
        "    train_loss_raw = get_values(\"train/loss\")\n",
        "    if train_loss_raw is not None:\n",
        "        train_loss = pd.DataFrame({\n",
        "            \"step\": train_loss_raw[\"step\"].values,\n",
        "            \"loss\": train_loss_raw[\"value\"].values\n",
        "        })\n",
        "    else:\n",
        "        train_loss = None\n",
        "\n",
        "    # --- 2. VALIDATION LOSS ---\n",
        "    val_loss_raw = get_values(\"eval/loss\")\n",
        "    if val_loss_raw is not None:\n",
        "        val_loss = pd.DataFrame({\n",
        "            \"step\": val_loss_raw[\"step\"].values,\n",
        "            \"loss\": val_loss_raw[\"value\"].values\n",
        "        })\n",
        "    else:\n",
        "        val_loss = None\n",
        "\n",
        "    # --- 3. ROUGE METRICS ---\n",
        "    def get_last_metric(tag):\n",
        "        m = get_values(tag)\n",
        "        return m[\"value\"].iloc[-1] if m is not None and not m.empty else None\n",
        "\n",
        "    rouge1 = get_last_metric(\"eval/rouge1\")\n",
        "    rouge2 = get_last_metric(\"eval/rouge2\")\n",
        "    rougeL = get_last_metric(\"eval/rougeL\")\n",
        "\n",
        "    # --- 4. GPU USAGE ---\n",
        "    gpu_raw = get_values(\"gpu_memory_gb\")\n",
        "    gpu_avg = gpu_raw[\"value\"].mean() if gpu_raw is not None else None\n",
        "\n",
        "    # --- 5. TRAINING TIME ---\n",
        "    tt_raw = get_values(\"total_training_time_seconds\")\n",
        "    train_time = tt_raw[\"value\"].iloc[0] if tt_raw is not None else None\n",
        "\n",
        "    # --- 6. HALLUCINATION ---\n",
        "    hall_path = os.path.join(run_path, \"hallucination.csv\")\n",
        "    if os.path.exists(hall_path):\n",
        "        hall_df = pd.read_csv(hall_path)\n",
        "        hallucination = hall_df[\"hallucination_rate\"].mean()\n",
        "    else:\n",
        "        hallucination = None\n",
        "\n",
        "    # --- FINAL OUTPUT ---\n",
        "    return {\n",
        "        \"run\": run_name,\n",
        "        \"train_loss\": train_loss,  # <--- always normalized DataFrame\n",
        "        \"val_loss\": val_loss,      # <--- always normalized DataFrame\n",
        "        \"rouge1\": rouge1,\n",
        "        \"rouge2\": rouge2,\n",
        "        \"rougeL\": rougeL,\n",
        "        \"gpu_avg\": gpu_avg,\n",
        "        \"train_time_sec\": train_time,\n",
        "        \"hallucination\": hallucination\n",
        "    }\n"
      ],
      "metadata": {
        "id": "P2Z_8aTs8cUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_metrics = []\n",
        "\n",
        "for run_name, run_path in RUNS.items():\n",
        "    metrics = extract_metrics(run_name, run_path)\n",
        "    print(metrics)\n",
        "    all_metrics.append(metrics)\n",
        "\n",
        "summary_df = pd.DataFrame([{\n",
        "    \"run\": m[\"run\"],\n",
        "    \"rouge1\": m[\"rouge1\"],\n",
        "    \"rouge2\": m[\"rouge2\"],\n",
        "    \"rougeL\": m[\"rougeL\"],\n",
        "    \"gpu_avg\": m[\"gpu_avg\"],\n",
        "    \"train_time_min\": m[\"train_time_sec\"] / 60 if m[\"train_time_sec\"] else None,\n",
        "    \"hallucination\": m[\"hallucination\"]\n",
        "} for m in all_metrics])\n",
        "\n",
        "summary_df\n"
      ],
      "metadata": {
        "id": "FbHxWoVz8eGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for m in all_metrics:\n",
        "    if m[\"train_loss\"] is None or m[\"val_loss\"] is None:\n",
        "        continue\n",
        "\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(m[\"train_loss\"][\"step\"], m[\"train_loss\"][\"loss\"], label=\"Train Loss\")\n",
        "    plt.plot(m[\"val_loss\"][\"step\"], m[\"val_loss\"][\"loss\"], label=\"Validation Loss\")\n",
        "\n",
        "    plt.title(f\"Loss Curve for {m['run']}\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "oTecMvgfA2mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "summary_df[\"data_size\"] = summary_df[\"run\"].map(RUN_SIZES)\n",
        "df_sorted = summary_df.sort_values(\"data_size\")\n",
        "\n",
        "\n",
        "# Sort by size to ensure correct order 2k → 10k\n",
        "df_sorted = summary_df.sort_values(\"data_size\")\n",
        "\n",
        "sizes = df_sorted[\"data_size\"].tolist()\n",
        "rouge1 = df_sorted[\"rouge1\"].tolist()\n",
        "rouge2 = df_sorted[\"rouge2\"].tolist()\n",
        "rougeL = df_sorted[\"rougeL\"].tolist()\n",
        "\n",
        "x = np.arange(len(sizes))  # number of groups\n",
        "width = 0.25               # width of each bar\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.bar(x - width, rouge1, width, label='ROUGE-1')\n",
        "plt.bar(x,         rouge2, width, label='ROUGE-2')\n",
        "plt.bar(x + width, rougeL, width, label='ROUGE-L')\n",
        "\n",
        "plt.xticks(x, [f\"{s//1000}k\" for s in sizes])\n",
        "plt.xlabel(\"Training Data Size\")\n",
        "plt.ylabel(\"ROUGE Score\")\n",
        "plt.title(\"ROUGE Scores for 2k vs 10k Training Samples\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1icaj5Xt_xRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}